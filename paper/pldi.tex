\documentclass[preprint,nocopyrightspace]{sigplanconf}
%\usepackage[square, comma, sort&compress]{natbib}
\usepackage{proof}

\usepackage{amsmath}
\usepackage{color}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\hide}[1]{}

\newcommand{\cd}[1]{\texttt{#1}}

\newcommand{\coq}[1]{\mathsf{#1}}
\newcommand{\return}[1]{\coq{return}(#1)}
\newcommand{\bind}{\leftarrow}
\newcommand{\emp}{\mathbf{emp}}
\newcommand{\sep}{\ast}
\newcommand{\pts}[2]{#1 \mapsto #2}
\newcommand{\new}[1]{\coq{new}(#1)}
\newcommand{\free}[1]{\coq{free}(#1)}
\newcommand{\rd}[1]{!#1}
\newcommand{\wri}[2]{#1 := #2}
\newcommand{\himp}{\Rightarrow}

\begin{document}

%\conferenceinfo{PLDI ’09}{ todo }

%\copyrightyear{2005}

%\copyrightdata{1-59593-056-6/05/0006}

\preprintfooter{DRAFT}

%\titlebanner{DRAFT}

\title{Effective Interactive Proofs for Higher-Order Imperative Programs}


\authorinfo{Double blind}
{ }

\maketitle

\begin{abstract}
  We present a new approach for constructing and verifying
  higher-order, imperative programs using the Coq proof assistant.
  Our approach is similar to the shallow embedding of Hoare Type
  Theory that was used in the Ynot project.  However, compared
  to Ynot, our new approach significantly reduces the burden on
  the programmer.  For example, in both systems we have constructed
  fully verified imperative data structures, such as hash-tables with
  higher-order iterators, but the verification burden in the new
  system is reduced by at least an order of magnitude compared to the
  old Ynot system, by replacing manual proof with automation.  The core of
  the automation is a simplification procedure for implications in
  higher-order separation logic, with hooks that allow programmers to
  add domain-specific simplification rules.
  
  Compared to competing approaches to data structure verification, our
  system includes much less code that must be trusted; namely, about a
  hundred lines of Coq code defining a program logic.  All of our
  theorems and decision procedures have or build machine-checkable
  correctness proofs from first principles, removing opportunities for
  tool bugs to create faulty verifications.  We argue for the
  effectiveness of our infrastructure by verifying a number of data
  structures and comparing to similar efforts within other projects.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\terms
%term1, term2

%\keywords
%keyword1, keyword2

\section{Introduction (GREG)}

A key goal of type systems is to prevent ``bad states'' from arising
in the execution of programs, but today's type systems fail to catch
language-level errors, such as a null-pointer dereference or an
out-of-bounds array index, much less library- and application-specific
errors such as removing an element from an empty queue, failing to
maintain the invariants of a blanced tree, or forgetting to release a
critical resource such as a database connection.  For safety- and
security-critical code, a type system should ideally rule out these
problems and, in the limit, make it possible for programmers to verify
that their code is correct.

There are many recent attempts to extend the scope of type systems to
address a wider range of safety properties.  Representative examples
include ESC/Java~\cite{esc-java}, Spec\#~\cite{spec-sharp},
ATS~\cite{ats}, Concoqtion~\cite{concoqtion}, Sage~\cite{sage}, and
Ynot~\cite{ynot:icfp}.  Each of these systems integrates some form of
specification logic into the type system in order to rule out a wider
range of truly bad states.

However, in the case of ESC/Java, Spec\#, and Sage, the program logic
is too weak to support full verification because these systems rely
upon provers to automatically discharge verification conditions.
While there have been great advances in the performance of automated
provers, in practice, they can only handle relatively shallow
fragments of first-order logic.  Thus, programmers are frustrated when
code that is correct is rejected by the type-checker.  For example,
none of these systems is able to prove that an array index is in
bounds when the constraints involved step outside quantifier-free
linear arithmetic.

In contrast, ATS, Concoqtion, and Ynot use powerful, higher-order
logics that support a much wider range of policies including (partial)
correctness.  Furthermore, in the case of Ynot, programmers can define
and use connectives in the style of separation logic~\cite{separation}
to achieve simple, modular specifications of higher-order imperative
programs.  For example, a recent paper describes how Ynot was used
to construct fully-verified implementations of data structures such
as queues, hash-tables, and splay-trees, including support for 
iterators.  

The price paid for these more powerful type systems is that, in
general, programmers must provide explicit proofs to convince the
type-checker that code is correct.  Unfortunately, explicit proofs can
be quite large when compared to the code.  For example, in the Ynot
module implementing a linked-list queue, the code takes up 7 lines,
whereas the proof of correctness is about 70 lines.

This paper reports our experience re-designing and re-implementing
Ynot to dramatically reduce the burden of writing and maintaining the
necessary proofs for full verification.  Like the original Ynot, our
system is based on the ideas of Hoare Type Theory~\cite{htt} and is
realized as an axiomatic extension of the Coq proof
assistant~\cite{CoqArt}.  This allows us to inherit the full power of
Coq's dependent types for writing code, specifications, and proofs,
and it allows us to use Coq's facility for extraction to executable ML
code.  However, unlike the previous version, we have taken advantage
of Coq's tactic language Ltac to implement a new set of parameterized
semi-decision procedures for automatically discharging or at least
simplifying the separation logic-style verification conditions.  The
careful design of these procedures makes it possible for programmers
to teach the prover about new domains as they arise.

We describe this new implementation of Ynot and report on our
experience implementing and verifying various imperative data
structures including stacks, queues, hash tables, binomial trees, and
binary search trees.  When compared with the previous version of Ynot,
we observe roughly an order of magnitude reduction in proof size.  In
most cases, to realize automation, programmers need only prove key
lemmas regarding the abstractions used in their interfaces and plug
these lemmas into our extensible tactics.  More importantly, we show
that the tactics used to generate the proofs are robust to small
changes in the code or specifications.

We also provide a qualitative comparison of our approach to two other
verification projects that have recently appeared in the literature,
Jahob~\cite{jahob} and Smallfoot~\cite{smallfoot}.  To summarize,
Jahob is based on the ideas of ESC/Java and Spec\# but aims for full
verification of imperative data structures.  In general, we found that
it requires many more annotations on the code because of its
``large-footprint'' approach to specifications.  It also requires a
much larger trusted computing base as it combines a number of external
decision procedures, some of which do not generate
separately-checkable proof witnesses justifying their conclusions.
Smallfoot, like Ynot, is based on separation logic and requires many
fewer annotations, but can only verify memory safety and not full
correctness.  Furthermore, Smallfoot only understands a small set of
hard-coded abstractions (linked list segments and tree segments),
while our new system supports sound extension of the library of
abstractions by programmers.

\section{The Ynot Programming Environment}

To a first approximation, Coq can be thought of as a functional programing language like Haskell or ML, but with support for dependent types.  For instance, one can have types such as:
\begin{verbatim}
div : nat -> forall n : nat, n <> 0 -> nat
\end{verbatim}
and write functions like
\begin{verbatim}
Definition avg (x:list nat) : nat :=
  let sum := fold plus 0 x in
  let len := length x in
  match eq_nat_dec len 0 with
    | inl(pf1: len = 0) => 0
    | inr(pf2: len <> 0) => div sum len pf2
  end.
\end{verbatim}

This function averages the values in a list of natural numbers.  It has a normal type like you might find in ML, and its implementation begins in an ML-like way, using a higher-order \cd{fold} function.  The interesting part is the \cd{match} expression.  We match on the result of a call to \cd{eq\_nat\_dec}, a dependently-typed natural number comparison function.  That function returns more than just a boolean; it returns a sum type with equality proofs in one branch and inequality proofs in the other.  We bind a name for each proof explicitly in the pattern for each \cd{match} case.  The proof that \cd{len} is not zero is passed to \cd{div} to justify the safety of the operation.

All Coq functions have to be pure -- terminating without side effects.  This is necessary to ensure that proofs really are proofs, with no spurious ``proofs by infinite loop.''  Ynot extends Coq with support for side-effecting computations.  Similar to Haskell, we introduce a monadic type constructor \cd{ST T} which describes computations that might diverge and that might have side effects, but that, if they do return, return values of type \cd{T}.  We can use \cd{ST} to safely keep the ``effectful'' computations separate from the pure computations.

Unlike Haskell's \cd{IO} monad, the \cd{ST} type is parameterized by a pre- and post- condition, which can be used to describe the effects of the computation on a mutable store.  Alternatively, one can think of the axiomatic base of Ynot as a fairly standard Hoare logic.  The main difference of our logic from usual presentations is that it is designed to integrate well with Coq's functional programming language, so that we formalize a language of expressions instead of commands, hence the IO-monad-like presentation.  A program derivation is of the form $\{P\} \; e \: \{Q\}$, where $P$ is a precondition predicate over heaps, and $Q$ is a postcondition predicate over initial heaps, functional values of $e$, and final heaps.  For instance, we can derive
$$\{\lambda \_. \; \top\} \; \return{1} \; \{\lambda h, v, h'. \; h' = h \land v = 1\}$$
and
$$\begin{array}{c}
  \{\lambda h. \coq{sel}(h, p_1) = p_2\} \\
  x \bind \; \rd{p_1}; \wri{x}{1} \\
  \{\lambda h, \_, h'. \; \coq{sel}(h, p_1) = p_2 \land h' = \coq{upd}(h, p_2, 1)\}
\end{array}$$

In actuality, Ynot has no separate concepts of programs and derivations.  Rather, the two are combined into one dependent type family, whose indices give the specification of a program.  For instance, the type of the first example program above would be:
\begin{verbatim}
ST (fun _ => True) (fun h v h' => h' = h /\ v = 1)
\end{verbatim}

Heaps are represented as functions from pointers to dynamically-typed packages, which are easy to implement in Coq with an inductive type definition.  The pointer read rule enforces that the heap value being read has the type that the code expects.  The original Ynot paper~\cite{ynot:icfp} contains further details of the base program logic.


\subsection{A Derived Separation Logic}

\begin{figure*}
  $$\infer{\{\emp\} \; \return{v} \; \{\lambda v'. \; [v = v']\}}{}
  \quad \infer{\{P_1\} \; x \bind e_1; e_2 \; \{Q_2\}}{
    \{P_1\} \; e_1 \; \{Q_1\}
    & (\forall x, \{P_2(x)\} \; e_2 \; \{Q_2\})
    & (\forall x, Q_1(x) \himp P_2(x))
  }$$

  $$\infer{\{\emp\} \; \new{v} \; \{\lambda p. \; \pts{p}{v}\}}{}
  \quad \infer{\{\exists v, \pts{p}{v}\} \; \free{p} \; \{\lambda \_. \; \emp\}}{}$$

  $$\infer{\{\exists v, \pts{p}{v} \sep P(v)\} \; \rd{p} \; \{\lambda v. \; \pts{p}{v} \sep P(v)\}}{}
  \quad \infer{\{\exists v, \pts{p}{v}\} \; \wri{p}{v'} \; \{\lambda \_. \; \pts{p}{v'}\}}{}$$

  $$\infer{\{P\} \; e \; \{Q\}}{
    P \himp P'
    & \{P'\} \; e \; \{Q'\}
    & Q' \himp Q
  }
  \quad \infer{\{P \sep R\} \; e \; \{Q \sep R\}}{
    \{P\} \; e \; \{Q\}
  }$$

  \caption{\label{STsep}The main rules of the derived separation logic}
\end{figure*}

Direct reasoning about heaps leads to very cumbersome proof obligations, with many sub-proofs that pairs of pointers are not equal.  Separation logic~\cite{separation} is the standard tool for reducing that complexity.  The old Ynot system built a separation logic on top of the axiomatic foundation, and we do the same here.  As before, we introduce no new syntactic class of separation logic formulas.  Instead, we define functions that operate on arbitrary predicates over heaps, with the intention that we will only apply them on separation-style formulas.  Nonetheless, it can be helpful to think of our ``assertion language'' as defined by:
$$\begin{array}{rcl}
  P &::=& [p] \mid \emp \mid \pts{x}{y} \mid P \sep P \mid \exists x, P
\end{array}$$

For any pure Coq proposition $p$, $[p]$ is the heap predicate that asserts that $p$ is true and the heap is empty.  $\emp$ asserts that the heap is empty, and $\pts{x}{y}$ asserts that the heap contains only a mapping from $x$ to $y$.  $P_1 \sep P_2$ asserts that the heap can be broken into two heaps $h_1$ and $h_2$ with disjoint domains, such that $h_1$ satisfies $P_1$ and $h_2$ satisfies $P_2$.  Finally, we add existential quantification.

It is worth pointing out that we simplify the assertion language substantially by taking advantage of Coq's base language.  We do not need to include program variables in the logic, because any Coq variable may be included anywhere in a Coq development, including within one of our assertions.  The same argument allows us to avoid explicit mention of specification variables, sometimes also called ``ghost state'' variables.

We can write much more expressive formulas than in most systems based on separation logic.  Not only can any pure proposition be injected with $[\cdot]$, but we can also use arbitrary Coq computation to build impure assertions.  For instance, we can include calls to custom recursive functions that return assertions.  We need no special support in the assertion language to accommodate this, and Coq's theorem-proving support for reasoning about pattern-matching recursive functions can be brought to bear without modification.

This automatic importation of Coq features has some surprising and pleasant consequences.  We literally need no more of the standard separation logic connectives to verify a wide variety of data structures.  For instance, separation logic proofs often use several kinds of disjunction.  In our setting, we reduce this disjunction to pattern matching over datatypes.  We take advantage of Coq's computational rules for simplifying pattern matches, which need not mention heaps, making them easier to think about and promoting easier proof automation.

We do need a notion of implication, though we do not need to include it in assertions.  There is no need to introduce the ``magic wand'' of separation logic.  Rather, we just define an almost trivial operator $\himp$, which appears only at the top level of individual proof obligations.  The meaning of $p \himp q$ is that any heap satisfying $p$ also satisfies $q$.  The relevant aspects of the usual ``magic wand'' connective are built into the definition of what a valid program derivation is in the separation logic.

\medskip

What we have described so far is the same as in the original Ynot work, with the exception that that work used some additional separation connectives that we no longer need.  The big departure of our new system is that we define a more standard separation logic.  The old Ynot's separation logic included ``binary postcondtions'' that may refer to both the initial and final heaps.  This is in stark contrast to traditional separation logics, where all assertions are simple separation formulas, and all verification proof obligations are simple implications between such assertions.  The utility of that formalism has been born out in the wealth of tools that have used separation logic for automated verification.  In contrast, verifications with the old Ynot tended to involve at least tens of steps of manual proof per line of program code.

Why did the original Ynot use this nonstandard program logic?  The answer has to do with the need for an effective analogue of specification variables.  In traditional separation logic, specification variables are commonly used to ensure that parts of state are preserved by commands, when the same specification variable appears in both the precondition and postcondition of a derivation.  In contrast, the old Ynot used binary postconditions for the same purpose, asserting equations between parts of the pre and post heaps.

The final output of a Ynot program is an executable piece of OCaml or Haskell code, produced with Coq's program extraction facility.  We can use standard Coq variables as specification variables, but, by default, they will remain at runtime, reducing the efficiency of programs.  One of the main innovations of the new work we present involves a new way of encoding specification variables in Coq, such that they will be erased by the standard program extraction facility.  This makes it possible to use a more standard separation logic, with the corresponding decrease in the difficulty of automating proofs.  We will present this new technique by example in later subsections.

Figure \ref{STsep} presents the main rules of our separation logic.  The notable divergence from common formulations is in the use of existential quantifiers in the rules for freeing, reading, and writing.  These differences make sense because Ynot is implemented within a constructive logic.  A more standard, ``classical'' separation logic would, for instance, require that, in the rule for $\coq{free}$, the value $v$ pointed to by $p$ be provided as an argument to the proof rule.  In constructive logic, such a value can only be produced when it can be computed by an algorithm.  Not only that, but we would not be able to use any facts implied by the current heap assertion to build one of these witnesses, and perhaps the witness can only be proved to exist using such facts.  The explicit existential quantifier frees us to reason ``inside the assertion language'' in finding the witness.

One consequence is that the ``read'' rule must take a kind of explicit framing condition.  This condition is parameterized by the value being read from the heap, making it a kind of description of the ``neighborhood around that value'' in the heap.  More standard separation logics force the exact value being read to be presented as an argument to the proof rule, but here we want to allow verification of programs where the exact value to read cannot be computed from the pieces of pure data that are in scope.

\medskip

In the rest of this section, we will introduce the Ynot programming environment more concretely, via several examples.


\subsection{Verifying an Implementation of Imperative Stacks}

\begin{figure}
  \begin{verbatim}
Module Type STACK.
  Parameter t : Set -> Set.
  Parameter rep T : t T -> list T -> hprop.

  Parameter new T :
    STsep __ (fun s : t T => rep s nil).
  Parameter free T (s : t T) :
    STsep (rep s nil) (fun _ : unit => __).

  Parameter push T (s : t T) (x : T) (ls : [list T]) :
    STsep (ls ~~ rep s ls)
          (fun _ : unit => ls ~~ rep s (x :: ls)).
  Parameter pop T (s : t T) (ls : [list T]) :
    STsep (ls ~~ rep s ls) (fun xo : option T => ls ~~
            match xo with
              | None => [ls = nil] * rep s ls
              | Some x => Exists ls' :@ list T,
                            [ls = x :: ls'] * rep s ls'
            end).
End STACK.
  \end{verbatim}

  \caption{\label{Stack}The signature of an imperative stack module}
\end{figure}

Figure \ref{Stack} shows the signature of a Ynot implementation of the stack ADT.  The signature is expressed in Coq's ML-like module system.  Each implementation contains a type family \cd{t}, where, for any type \cd{T}, a value of \cd{t(T)} represents a stack storing values of \cd{T}.  An abstract type in ML can be thought of as ``standing for the underlying invariant.''  For Ynot, this is true for ``pure'' invariants only.  We need to add an additional ``impure'' invariant explicitly.  The type \cd{hprop} stands for assertions, and the parameter \cd{rep} is an assertion-valued function over a stack and a \emph{functional model} of the stack.  Each stack can be thought of as standing for a particular pure list, and the \cd{rep} predicate formalizes this connection.

The type of the \cd{new} operation tells us that it expects an empty heap on input, and on output the heap contains just whatever mappings are needed to satisfy the representation invariant between the function return value and the empty list.  The ASCII notation \cd{\_\_} stands for the $\emp$ of usual pencil-and-paper separation logics.  The \cd{free} operation takes a stack \cd{s} as an argument, and it expects the heap to satisfy \cd{rep} on \cd{s} and the empty list.  The post state shows that all heap values associated with \cd{s} are freed.

The specification for \cd{push} says that it expects any valid stack as input and modifies the heap so that the same stack that stood for some list \cd{l} beforehand now stands for the list \cd{x :: l}, where \cd{x} is the appropriate function argument.  We see an argument \cd{ls} with type \cd{[list T]}.  The brackets are a notation defined by the Ynot library, standing for \emph{computational irrelevance}.  That is, the type-checker should enforce that the value of \cd{ls} is not needed to execute the function.  Rather, such values may only be used in stating specifications and discharging proof obligations.  In other words, irrelevant variables act just like specification variables, but we do not need to build any special support for them into our assertion language.  We use Coq's notation scope mechanism to overload brackets for writing irrelevant types and lifted pure propositions.

For an assertion \cd{P} that mentions the irrelevant variable \cd{x}, the notation \cd{x \textasciitilde\textasciitilde \; P} must be used to explicitly ``unpack'' \cd{x}.  The type of the unpack operation is such that it may only be applied to assertions and may not be used to allow an irrelevant variable's value to ``leak into'' the computational part of a program.

The type of \cd{pop} showcases how we avoid the disjunctive connectives of separation logic.  The function returns an optional \cd{T} value, which will be \cd{None} when the stack is empty and will be \cd{Some x} when \cd{x} is at the top of the stack.  We use a Coq \cd{match} expression to give a different postcondition for each case.

\medskip

We can implement a module satisfying this signature.  With the type \cd{T} as a local variable, we can define the type of nodes of the linked lists that we will use.

\begin{verbatim}
Record node : Set := Node {
  data : T;
  next : option ptr
}.
\end{verbatim}

To define the representation invariant, we want a recursive function specifying what it means for a possibly-null pointer to represent a functional list.

\begin{verbatim}
Fixpoint listRep (ls : list T) (hd : option ptr)
    {struct ls} : hprop :=
  match ls with
    | nil => [hd = None]
    | h :: t => match hd with
                  | None => [False]
                  | Some hd => Exists p :@ option ptr,
                      hd --> Node h p * listRep t p
                end
  end.
\end{verbatim}

We can represent stacks as untyped pointers to the heads of linked lists built from \cd{Node}s.

\begin{verbatim}
Definition stack := ptr.
\end{verbatim}

We achieve type safety through the representation invariant.

\begin{verbatim}
Definition rep (s : stack) (ls : list T) : hprop :=
  Exists po :@ option ptr, s --> po * listRep ls po.
\end{verbatim}

Before we start implementing the ADT methods, we should set up some proof automation machinery.  Systems like Smallfoot~\cite{smallfoot} have hardcoded support for particular heap predicates like acyclic linked list-ness, cyclic linked list-ness, and so on.  These systems perform ad-hoc simplifications on formulas that mention the predicates that they understand.  In contrast, with Ynot, the programmer can define his own new predicates, as we have just done.  Not only that, but he can also prove lemmas that correspond with the simplification rules built into automated tools, and he can plug his lemmas into a general separation logic solver.  All of this is done with no risk that a mistake by the programmer will lead to a faulty verification; every lemma must be proved from first principles.

For the current example, we need two lemmas for ``unfolding'' the definition of \cd{listRep} at points where we know for sure whether or not the pointer argument is null.

\begin{verbatim}
Theorem listRep_None : forall ls,
  listRep ls None ==> [ls = nil].
  destruct ls; sep fail idtac.
Qed.

Theorem listRep_Some : forall ls hd,
  listRep ls (Some hd) ==> Exists h :@ T,
    Exists t :@ list T, Exists p :@ option ptr,
      [ls = h :: t] * hd --> Node h p * listRep t p.
  destruct ls; sep fail ltac:(try discriminate).
Qed.
\end{verbatim}

The proofs are given in Coq's tactic language.  We ask to do a case analysis on the structure of the list \cd{ls}, applying the parametrized separation logic solver in each case.  We will go into more detail shortly on the significance of the two arguments to the \cd{sep} procedure.

Now we can plug our two unfolding rules into the separation solver.  We define a local procedure for simplifying separation implications.

\begin{verbatim}
Ltac simp_prem :=
  simpl_IfNull;
  simpl_prem ltac:(apply listRep_None
                   || apply listRep_Some).
\end{verbatim}

Our procedure, or \emph{tactic} in Coq parlance, first calls a simplification procedure associated with a syntax extension for checking pointer nullness.  Next, our procedure calls a tactic \cd{simpl\_prem} from the Ynot library, for simplifying premises of implications.  The argument to \cd{simpl\_prem} gives a procedure to attempt on each premise, until no further progress can be made.

We can plug our domain-specific simplification tactic into the generic separation logic solver.  We define a new tactic that will suffice to prove all goals that will arise in this example.  The first argument to \cd{sep} gives a procedure to apply to simplify premises before starting the main proof search, and the second argument gives a procedure to attempt on individual subgoals throughout proof search.  The \cd{discriminate} tactic solves goals whose premises include inconsistent equalities over values of datatypes, like \cd{nil = x :: ls}; and adding \cd{try} in front prevents \cd{discriminate} from signaling an error if no such equality exists.

\begin{verbatim}
Ltac t := unfold rep; sep simp_prem
                          ltac:(try discriminate).
\end{verbatim}

We implement each method by stating its type as a proof search goal, using tactics to realize the goal step by step.  Here is the implementation of the \cd{new} operation.

\begin{verbatim}
Definition new : STsep __ (fun s => rep s nil).
  refine {{New (@None ptr)}}; t.
Qed.
\end{verbatim}

A simple two-step proof script suffices.  We first use the \cd{refine} tactic to provide a ``template'' for the implementation.  The template may have holes in it, and each hole is added as a subgoal.  We chain our \cd{t} tactic with the semicolon operator, so that \cd{t} is applied to each subgoal generated from a hole.  This is enough to finish the implementation and proof.

The holes in the refinement are not apparent from the syntax we have used.  We apply Coq's syntax extension system to hide such details for most programs.  In this case, it is the \cd{\{\{...\}\}} syntax that hides the holes.  This syntax requests simultaneous strengthening and weakening.  It expands to an explicit call to a rule that takes two proofs as arguments.  Those arguments are filled in as holes by the syntax extension.

The definitions of \cd{free} and \cd{push} are not much more complicated.

\begin{verbatim}
Definition free s : STsep (rep s nil)
    (fun _ : unit => __).
  intros; refine {{Free s}}; t.
Qed.
\end{verbatim}

\begin{verbatim}
Definition push s x ls : STsep (ls ~~ rep s ls)
    (fun _ : unit => ls ~~ rep s (x :: ls)).
  intros; refine (hd <- !s;
    nd <- New (Node x hd);
    {{s ::= Some nd}}
  ); t.
Qed.
\end{verbatim}

The implementation of \cd{pop} uses another syntax extension, which provides an \cd{IfNull} expression form.  The \cd{option}-typed argument to \cd{IfNull} is checked for nullness (i.e., equality to \cd{None}).  In an \cd{Else} branch, where the pointer is known to be non-null, that fact is added as a usable proof hypothesis, and the variable being tested is rebound with a non-\cd{option} type.

\begin{verbatim}
Definition pop s ls :
  STsep (ls ~~ rep s ls) (fun xo => ls ~~
          match xo with
            | None => [ls = nil] * rep s ls
            | Some x => Exists ls' :@ list T,
                          [ls = x :: ls'] * rep s ls'
          end).
  intros; refine (hd <- !s;
    IfNull hd Then
      {{Return None}}
    Else
      nd <- !hd;
      Free hd;;
      s ::= next nd;;
      {{Return (Some (data nd))}}); t.
Qed.
\end{verbatim}

We complete the implementation with a trivial definition of the type family \cd{t}, relying on the representation invariant to ensure proper use.

\begin{verbatim}
Definition t (_ : Set) := stack.
\end{verbatim}

\begin{figure}
  \begin{verbatim}
module Stack = 
 struct 
  type 't node = { data : 't; next : ptr option }
  let data n = n.data
  let next n = n.next
  type stack = ptr
  
  let coq_new =
    sepWeaken (sepStrengthen (sepFrame (sepNew None)))
  
  let free s =
    sepWeaken (sepStrengthen (sepFrame (sepFree s)))
  
  let push s x =
    sepBind (sepStrengthen (sepRead s)) (fun hd ->
      sepBind (sepStrengthen (sepFrame
          (sepNew { data = x; next = hd })))
        (fun nd ->
        sepWeaken (sepStrengthen (sepFrame
          (sepWrite s (Some nd))))))
  
  let pop s =
    sepBind (sepStrengthen (sepRead s)) (fun hd ->
      match hd with
        | Some v ->
            sepBind (sepStrengthen (sepRead v)) (fun nd ->
              sepSeq (sepStrengthen (sepFrame (sepFree v)))
                (sepSeq (sepStrengthen (sepFrame
                    (sepWrite s (next nd))))
                  (sepWeaken
                    (sepStrengthen (sepFrame
                      (sepReturn (Some (data nd))))))))
        | None -> sepWeaken (sepStrengthen (sepFrame
                    (sepReturn None))))
  
  type 'x t = stack
 end
  \end{verbatim}

  \caption{\label{extracted}OCaml code extracted from the stack example}
\end{figure}

For our modest efforts, we can now extract an executable OCaml version of our module.  Figure \ref{extracted} shows the result of running Coq's automatic extraction command on our \cd{Stack} module.

In the method implementations, we see invocations of functions whose names begin with \cd{sep}.  These come from the Ynot library, and we must provide their OCaml implementations.  Any Ynot program that returns a type \cd{t} may be represented in \cd{unit -> t} in OCaml, regardless of the specification appearing in the original Coq type.  This makes it easy to implement the basic functions, in the spirit of how the Haskell IO monad is implemented.  We see calls to explicit weakening, strengthening, and framing rules in the extracted code.  In OCaml, these can be implemented as no-ops and elided by an optimizer.

Notice that all specification variables and proofs are eliminated automatically by the Coq extractor.  With the elision of weakening and related operations, we arrive at exactly the kind of monadic code that is standard fare for Haskell, such that the body of optimizations developed for Haskell can be put to immediate use in creating an efficient compilation pipeline for Ynot.


\subsection{Verifying Imperative Queues}

It is not much harder to implement and verify a queue structure.  We define an alternate list representation, parameterized by head and tail pointers.

\begin{verbatim}
Fixpoint listRep (ls : list T) (hd tl : ptr)
    {struct ls} : hprop :=
  match ls with
    | nil => [hd = tl]
    | h :: t => Exists p :@ ptr, hd --> Node h (Some p)
                  * listRep t p tl
  end.

Record queue : Set := Queue {
  front : ptr;
  back : ptr
}.

Definition rep' (ls : list T) (fr ba : option ptr) :=
  match fr, ba with
    | None, None => [ls = nil]
    | Some fr, Some ba => Exists ls' :@ list T,
        Exists x :@ T, [ls = ls' ++ x :: nil]
          * listRep ls' fr ba * ba --> Node x None
    | _, _ => [False]
  end.
          
Definition rep (q : queue) (ls : list T) :=
  Exists fr :@ option ptr, Exists ba :@ option ptr,
    front q --> fr * back q --> ba * rep' ls fr ba.
\end{verbatim}

For this representation, we prove similar ``unfolding'' lemmas to those we proved for stacks, with comparable effort.  We also need a new lemma for ``unfolding a queue from the back.''

\begin{verbatim}
Lemma rep'_back : forall ls fr ba,
  rep' ls (Some fr) ba
  ==> Exists nd :@ node, fr --> nd
    * Exists ls' :@ list T, [ls = data nd :: ls']
      * match next nd with
          | None => [ls' = nil]
          | Some fr' => rep' ls' (Some fr') ba
        end.
\end{verbatim}

The proof of the lemma relies on some lemmas about pure functional lists.  With those available, we prove \cd{rep'\_back} in under 20 lines.  When we plug this and the two other unfolding lemmas into the \cd{sep} procedure, we arrive at quite a robust decision procedure for separation assertions about lists that may be modified at either end.

Again, every proof obligation for our queue implementation is proved by a \cd{t} tactic built from \cd{sep}.  We write under 10 lines of new tactic hints to be applied during proof search, and we must prove one key lemma by induction.

\begin{verbatim}
Lemma push_listRep : forall ba x nd ls fr,
  ba --> Node x (Some nd) * listRep ls fr ba
  ==> listRep (ls ++ x :: nil) fr nd.
  Hint Resolve himp_comm_prem.
  induction ls; t.
Qed.
\end{verbatim}

We suggest a hint based on commutativity of separating conjunction, state the inductive structure of the proof, and discharge the two cases with our specialized \cd{t}.  This is an instance of a common pattern, where lemmas requiring inductive proofs must be stated explicitly, while most other separation facts are proved automatically by \cd{sep}.


\subsection{Arrays}

It is also easy to support arrays within our framework.  We can define suitable constants in the base program logic:

\begin{verbatim}
Parameter array : Set.
Parameter array_length : array -> nat.  
Parameter array_plus : array -> nat -> [ptr].
\end{verbatim}

The \cd{array\_plus} function is for computing a pointer to a particular cell of an array.  Note how the type of this function forces the resulting pointer to be treated in a computationally irrelevant way.  That is, it is easy to reason about the separate cells of an array with the separation logic connectives we already introduced, but client code cannot actually calculate pointers from arrays ``at runtime.''  This makes it possible to represent Ynot arrays with OCaml arrays.

With an infix notation \cd{@+} defined for \cd{array\_plus}, it is easy to write a type for array update.  We use the notation \cd{x :\textasciitilde\textasciitilde \; v in P} to bind \cd{x} to the hidden value of specification value \cd{v} within the assertion \cd{P}.  We use the shorthand \cd{p -->?} to indicate that \cd{p} points to some unknown value of unknown type.

\begin{verbatim}
Parameter upd_array (A:Type)(a:array)(i:nat)(v:A) :
  STsep (p :~~ a @+ i in p -->?)
        (fun _ : unit =>
               p :~~ a @+ i in p --> v).
\end{verbatim}

The type of the array allocation operation indicates that every pointer within the array is initialized to some unknown value.  The notation \cd{\{@ e | i <- min + len\}} is inspired by standard set comprehension notations, and it denotes an iterated separating conjunction where the variable \cd{i} ranges from \cd{min} to \cd{min + len - 1}.

\begin{verbatim}
Parameter new_array (n:nat) :
  STsep __ (fun a : array => [array_length a = n]
    * {@ p :~~ a @+ i in p -->? | i <- 0 + n}.
\end{verbatim}


\subsection{Loops}

Like with many semi-automated verification systems, we require annotations that are equivalent to ``loop invariants.''  Since Coq's programming language is functional, it is more natural to write ``loops'' as recursive functions, and the ``loop invariants'' become the pre- and postconditions of these functions.

We support general recursion with a primitive fixpoint operator in the base program logic, and it is easy to build a separation logic version on top of that.  We can also build multiple-argument recursive function forms on top of the single-argument form.

An example is a \cd{getElements} function, defined in terms of the list invariant that we wrote for the stack example.  This operation returns the functional equivalent of an imperative list.  The task is not so trivial as it may look at first, because the computational irrelevance of the function's second argument prohibits its use to influence the return value.

\begin{verbatim}
Definition getElements (hd : option ptr) (ls : [list A]) :
  STsep (ls ~~ listRep ls hd)
        (fun res : list A => ls ~~ [res = ls]
          * listRep ls hd).
  refine (Fix2
    (fun hd ls => ls ~~ listRep ls hd)
    (fun hd ls res => ls ~~ [res = ls] * listRep ls hd)
    (fun self hd ls => 
      IfNull hd Then
        {{Return nil}}
      Else
        fn <- !hd;
        rest <- self (next fn) (ls ~~~ tail ls) <@> _;
        {{Return (data fn :: rest)}})); t.
Qed.
\end{verbatim}

The code demonstrates a use of one of the derived fixpoint combinators, \cd{Fix2}.  Of the three arguments that we pass, the first two give the precondition and postcondition in terms of the two ``real'' arguments (and, for the postcondition, the return value).  The third argument is the function body.  It takes a recursive self-reference as its first argument, followed by the two ``real'' arguments.

The notation \cd{x \textasciitilde\textasciitilde\textasciitilde \; e} is for building a new computationally-irrelevant value out of an old one.  The notation \cd{e <@> P} is explicit invocation of the frame rule.  With the current system, one usually wants to invoke that rule at each function call.  The framing assertion can be written as an underscore to ask that it be inferred.  We would like to avoid such explicit framings, and they only remain because it is unclear how to implement our syntax extensions in a way that avoids them.


\section{Tactic Support}

The examples from the last section show how much of the gory details of proofs can be hidden from programmers.  In actuality, every command triggers the addition of one or more proof obligations that cannot be discharged effectively by any of the built-in Coq automation tactics.  Not only is it hard to prove the obligations, it is also hard to infer the right intermediate specifications.  Our separation logic formulas range well outside the propositional fragment that automated tools tend to handle; specification inference and proving must deal with higher-order features.

Here is an example of the proof obligations generated for the code we gave earlier for the stack \cd{push} method.  Numbers prefixed with question marks are unification variables, whose values the \cd{sep} tactic must infer.

\begin{verbatim}
 ls ~~ rep s ls ==>
 Exists v :@ option ptr, s --> v * ?200 v

 forall v : option ptr, s --> v * ?200 v ==> ?192 v

 ?192 hd ==> ?217 * __

 forall v : ptr, ?217 * v --> Node x hd ==> ?206 v

 ?206 nd ==> ?234 * (Exists v' :@ ?231, s --> v')

 ?234 * s --> Some nd ==> ls ~~ rep s (x :: ls)
\end{verbatim}

We can see that each goal has at most one new unification variable standing for a specification, and each such variable has its value determined by the value of the variable from the previous goal.  This is no accident; we designed our combinators and notations to have this property.

Now we are ready to describe what the \cd{sep} tactic does.  We do not have space to include the literal Coq code implementing it; we will give pseudocode instead.  The implementation is in Coq's Ltac language~\cite{Ltac}, a domain-specific, dynamically-typed language for writing proof-generating proof search procedures.  Here is an example Ltac program that implements simplifications of goals that contain conjunctions.

\begin{verbatim}
Ltac un_and :=
  repeat match goal with
           | [ H : _ /\ _ |- _ ] => destruct H
           | [ |- _ /\ _ ] => split
         end.
\end{verbatim}

This program repeats a pattern-matching process until it fails.  Each iteration looks first for a hypothesis that states a conjunction.  If one is found, the tactic \cd{destruct} is used to split it into two new hypotheses.  If no conjunctive hypotheses remain, the second \cd{match} case checks if the conclusion is a conjunction.  If so, the \cd{split} tactic is used to split the goal into one subgoal for each conjunct.

\medskip

As we have seen, \cd{sep} takes two arguments, which we will call \cd{unfolder} and \cd{solver}.  The task of \cd{unfolder} is to simplify premises before specification inference, usually by unfolding definitions of recursive predicates, based on known facts about their arguments.  The task of \cd{solver} is to solve all of the goals that remain after generic separation logic reasoning is applied.

Coq comes with the standard tactic \cd{tauto}, for proving propositional tautologies.  There is a more general version of \cd{tauto} called \cd{intuition}, which will apply a user-supplied tactic to finish off sub-proofs, while taking responsibility for handling propositional structure on its own.  The \cd{intuition} tactic also has the helpful property of leaving for the user any subgoals that it could not establish.  \cd{sep} is meant to be an analogue of \cd{intuition} for separation logic.  We also want it to handle easy instantiation of existential quantifiers, since they appear so often in our specifications.

This is the procedure that \cd{sep} follows:

\begin{enumerate}
\item Look for an easy case of specification inference.
  \begin{enumerate}
  \item If a unification variable $U$ appears alone in the implication conclusion, set $U$ equal to the premise.
  \item If the conclusion is a unification variable $U$ applied to a program variable $x$, set $U$ equal to a function that abstracts over the occurrences of $x$ in the premise.
  \item Apply similar logic for cases like the above where extra $\emp$s appear as conjuncts around the main conclusion formula.
  \end{enumerate}
\item If this was not an easy case, try harder.
  \begin{enumerate}
  \item Introduce names for all existentially-quantified variables in the premise.
  \item Introduce names for the values hidden inside of all computationally-irrelevant variables in scope.
  \item Try applying \cd{unfolder}.  If it succeeds, repeat from the introduction of names.
  \item Check if the conclusion matches the precondition of the rule for memory reads, with a unification variable $U$ appearing for the frame assertion.  If so, pick out from the conclusion the pointer $p$ that is being read.  Search syntactically in the premise for a points-to fact $\pts{p}{v}$.  ``Cross out'' that fact, and set $U$ equal to a function that abstracts over all occurrences of $v$ in what remains of the premise.  The function must additionally re-quantify any variables coming from existential quantifiers, and it must explicitly unpack any computationally-irrelevant variables that appear.
  \item If the last rule did not match, check if the conclusion matches the precondition of the rule for memory writes for some pointer $p$, with an extra unification variable $U$ added as a frame assertion.  Like in the last case, pick out a premise assertion $\pts{p}{v}$ and remove it.  Set $U$ equal to everything that remains of the premise, modulo the same re-quantification as in the last case.
  \item If the previous two rules did not match, cancel out equal subformulas on both sides of the implication.  If only a unification variable $U$ remains in the conclusion, set it equal to the remaining premise, after re-quantification.
  \end{enumerate}
\item Loop through attempting the following premise-oriented simplifications, until no more apply.
  \begin{enumerate}
  \item Apply standard Coq automation tactics, covering propositional structure and simplification of pattern-matching and calls to recursive functions.
  \item Apply \cd{solver}.
  \item Remove an occurrence of $\emp$ from the premise.
  \item Remove a pure fact $[p]$ from the premise, adding it as a hypothesis to the normal proof context.
  \end{enumerate}
\item Loop through attempting the following conclusion-oriented simplifications, until no more apply.  Try the first two steps of the last loop after each simplification.
  \begin{enumerate}
  \item Replace an existentially-quantified formula with its body, where the bound variable is replaced by a fresh unification variable.
  \item Cancel out a pair of occurrences of the same subformula on both sides of the implication.  The unification that occurs in this step can induce settings for unification variables introduced by the previous rule.
  \item Remove an occurrence of $\emp$ from the conclusion.
  \item Find an occurrence of a pure fact $[p]$ in the conclusion, prove it with \cd{solver}, and remove it from the conclusion.
  \end{enumerate}
\item If the sides of the implication match, finish the proof by reflexivity.
\end{enumerate}

Every step of this process is implemented in Ltac, so that only a bug in Coq would allow \cd{sep} to declare an untrue goal as true.  By construction, every step builds an explicit proof term, and we can validate all proof terms afterward with an independent checker that is relatively simple, compared to the operation of all of the decision procedures that may have contributed to the proof.


%% \section{Comparison to Previous Ynot (AVI)}

%%  The original formulation of Ynot lead to large, unwieldy proofs.
%%  Reasoning about separation logic connectives was done at the heap
%%  level; a larger fraction of all of our proof scripts simply massaged
%%  subeaps into appropriate forms.  We had some tactics that implemented
%%  basic operations, such as the split, join, and flattening of
%%  sub-heaps, but the proof writer was forced to stitch them together to
%%  force the heaps into the correct form.

%%  The focus of our re-implementation of the Ynot system has been proof
%%  automation.  Throught the use of the sep tactic, we have virtually
%%  eliminated the need to reason about heaps; reasoning is done at the
%%  level of seperation logic.  Much of this reasoning has in turn been
%%  automated by a set of tactics such as \texttt{sep auto}. These
%%  tactics reduce the need to reason about the connectives of separation
%%  logic and their properties, allowing the proof writer to focus on the
%%  domain specific parts of the proof.  For proofs that only have simple
%%  domain specific parts, \texttt{sep auto} is often able to prove them
%%  on its own.

%%  The new model uses ``ghost variables'' to simplify specifying
%%  properties. It does so in a way that guarantees that they are
%%  computationally irrelevant, and so can always be eliminated when a
%%  program is extracted.  This in turn allows the new model to use unary
%%  post-conditions, which greatly simplifies specifications and enables
%%  the greater automation.  The old model used binary post-conditions to
%%  obviate the need for ghost variables.  Binary post-conditions where
%%  generally harder to reason about, since the connection between pre-
%%  and post-conditions always needed to be re-established.  In the new
%%  style, the connection is built in to the specification.

%%  In the new Ynot, proofs are an order of magnitude shorter then they
%%  where in the old Ynot. (provide numbers here)

\section{Evaluation}

\begin{figure*}
  \begin{center}
    \begin{tabular}{r | r | r | r | r | r | r | r}
      & Program & Total Non-Program & Specs & Annotations & Aux. Defs. & Proofs & Tactics\\\hline
      Linked list finite map & AVI\\
      Association list Jahob comparison & RYAN\\
      Linked list Smallfoot comparison & GREGORY\\
      Hash table & AVI\\
      Binomial tree & RYAN\\
      Binary search tree finite map & AVI\\
      Stack & 13 & 31 & 8 & 0 & 9 & 9 & 5 \\
      Queue & 34 & 81 & 12 & 0 & 14 & 32 & 23 \\
    \end{tabular}
  \end{center}

  \caption{\label{loc}Breakdown of numbers of lines of different kinds of code in the case studies}
\end{figure*}

Figure \ref{loc} presents code size statistics for the case studies that we have completed in our new system.  The column ``Program'' counts the number of lines that have equivalents in extracted ML programs, ignoring module system plumbing.  ``Total Non-Program'' gives the sum of all columns that follow it.  ``Specs'' includes type signatures and pre- and postconditions for the methods.  ``Annotations'' includes additional annotations within function definitions, included only for proof purposes.  ``Aux. Defs.'' covers auxiliary definitions of notions like representation predicates.  ``Proofs'' includes statements and proofs of example-specific lemmas, and ``Tactics'' includes example-specific coding of Ltac semi-decision procedures.


\section{Related Work}
\subsection{Jahob (RYAN)}

The Jahob system~\cite{jahob} allows the specification and verification 
of recursive, linked data structures in a fragment of Java.  Like ynot, 
verified programs are correct up to termination.  We compare Ynot
and Jahob along two lines: representation (underlying logic, heap model, abstract state),
 and reasoning (how programs are verified).  We illustrate using a running
 example of an association list, implemented similarly in Jahob and Ynot.
 
Jahob specifications are written in classical higher order logic.  Ynot
specifications, while higher order, cannot currently be written in classical logic. The law of the excluded middle implies proof irrelevence, which 
conflicts with a Ynot axiom required for reasoning about computationally
irrelevent functional models.  However, we believe a slight modification 
to Coq should remedy this. 

The abstract state of an object in Jahob is given by s a collection of 
programmer defined ``specvars'' which do not exist during program execution.  These are 
equivalent to Ynot's pure functional models and \cd{rep} predicates.

Specvars are defined with formulae that resemble set 
theory in Isabelle/HOL.  Jahob formulae are simply typed with ground types
bool, obj, and int, type constructors $\Rightarrow$ for total functions, * for tuples
and set for sets.  The logic contains polymorphic equality, standard
logical connectives $\wedge, \vee, \neg, \to, \forall, \exists$ and
$\lambda$ binders, set comprehension, operations on sets ($\cup, \in$) 
and transitive closure, finite set cardinality, and a tree function for indicating
that a structure is a tree.  This set of operators is fixed and cannot be extended
by the programmer.  Specifications are written in Java comments.  

In Jahob, an association list based can be given an interface that includes:

\begin{verbatim}
class AssocList {
//: public specvar content :: "(obj * obj) set
public Object put(Object k0, Obkect v0)
/*: requires "k0 <> null /\ v0 <> null"
    modifies content
    ensures  
    "content = old content 
               - {(k0, result)} U {(k0, v0)} 
     /\ (result =  null -> 
          ~ exists v, 
             (k0, v) in old content) /\
     /\ (result <> null -> 
           (k0, result) in old content) */
{...} }
\end{verbatim}

Like ynot, Jahob relates the abstract state of a data structure to its concrete heap representation.  
One way to implement the association list with a singly linked list is to represent links between 
nodes using an edge relation (vardef is a shorthand definition):

\begin{verbatim}
public /*: claimedBy AssocList */ 
class Node {
    public Object key; public Object value; 
    public Node next;
    //: public ghost specvar cnt 
         :: "(obj * obj) set" = "{}"   }
         
private static specvar edge 
  :: ``obj => obj => bool'';

vardefs ``edge == (fun x y => 
  (x in Node /\ y = x..next) \/
  (x in AssocList /\ y = x..first))'';
invariant InjInv: forall x1 x2 y,
   y <> null /\ edge x1 y 
   /\ edge x2 y -> x1 = x2'';
\end{verbatim} 

The (recursive) representation invariant defines the members of the abstract set 
representing the linked list as the union of the element in the head node 
and the members of the subsequent nodes.  It also ensures that keys only occur once in the list.

\begin{verbatim}
private Node first;
vardefs "content == first..cnt";

invariant CntDef:
    ``forall x, x in Node /\ x in alloc 
        /\ x <> null ->
           x..cnt = {(x..key, x..value)} 
              U x..next..cnt /\
           (forall v, (x..key, v) 
              notin x..next..cnd)'';
              
invariant CntNull:
    ``forall x, x in Node /\ x in Alloc 
        /\ x = null -> x..cnt = {}'';
\end{verbatim}

The set-theoretic nature of Jahob specifications gives reasoning a slightly
different feel than Ynot.  Set extensionality must be built on top of
the definitional equality of Coq, and so our equivalent Ynot implementation
uses lists with a uniqueness invariant to provide the semantic guarantees 
of the Jahob interface.  

Although Jahob provides a number of constructs to simplify
the set theoretic description of linked data structures, reasoning about
the heap is done using conventional techniques that can be difficult
to scale (todo cite).  Ynot uses separation logic to both simply
 reasoning about the heap and to provide a useful ``heap abstraction
barrier'' to programmers.  The effectiveness of the sep tactic
is evidence of the scalability of our approach.
 
Whereas Ynot's automated reasoning is completely captured by Coq tactics,
Jahob programs are verified using a series of stages similar to 
a compiler pipeline.  First, Jahob programs are translated into 
an extended guarded command language.
Then, Jahob generates verification conditions which are discharged 
using a combination of theorem provers -- if one prover does not succeed 
on an obligation, another is tried.  This allows Jahob to run multiple
provers in parallel.  Because provers are often specialized to particular
classes of formulae, formula approximation techiques must be used to integrate them with
a system using higher order logic. Jahob supports a number of 1st order and SMT provers, and also the 
MONA prover for the monadic fragment of 2nd order logic and Isabelle and Coq.  
The total amount of Jahob code is (todo), and Ynot code is (todo).

We have compared the Jahob association list with an equivalent in ynot program. 
Jahob uses 27 lines of ``representation'' code -- things like the invariant
and edge relation.  The ynot association list has only 20 lines of such code,
but also requires 8 lines to model unique lists and 100 lines of ``reasoning'' code -- things
like lemmas and tactics.  The vast majority of this code is infrastructure code
which can presumably be shared using libraries.  In Jahob, the equivalent functionality
is provided by the underlying logic and theorem provers.  The Jahob program
requires 17 lines of annotations, and the Ynot program 26 lines.   The Ynot
program compiles in about two minutes on a 2ghz laptop.

 \subsection{Smallfoot (GREGORY)}
We conclude our comparison with alternative approaches by comparing
our system to Smallfoot~\cite{smallfoot}. The focus of Smallfoot is on
completely automatic reasoning and concurrency. Since the current
version of Ynot does not address concurrency, we will not address this
in our comparison. We draw a comparison in two parts: the size of
annotations and proof obligations and the expressivity of the
logic. We use the linked list as a running example.

The Smallfoot system defines a segment of a linked list as the minimal
predicate satisfying the following:
$$
ls(E,F) \Leftrightarrow (E = F \wedge \mathtt{emp}) \vee (E \neq F \wedge \exists y. E \mapsto tl:y * ls(y,F))
$$

\noindent The definition is expressible in a similar manner to the
queue representation fixpoint described previously. The notable
difference being the addition of the inequality which is necessary to
guarantee non-cycles. The only difficulty in reasoning comes from the
fact that {\tt new} returns a pointer which is guaranteed distinct
only from {\it currently allocated} pointers, not all previously
allocated pointers. This requires the Ynot functions which allocate to
accept a ghost variable witnessing that the list ends at {\tt nil} or
an allocated pointer. The type for {\it cons} shows the use of this
condition.
\begin{verbatim}
cons :: forall (v : A) (r : LinkedList) 
  (q : [LinkedList]) (m : [list A]) (T : Type)
  (vt : [T]),
  STsep (q ~~ m ~~ vt ~~ llseg r q m * q ~~> vt)
    (fun r:LinkedList => q ~~ m ~~ vt ~~ 
     llseg r q (v :: m) * q ~~> vt * [r <> None]).
\end{verbatim}
Where the {\tt ~~>} arrow denotes a option pointer (potentially null)
pointing to {\it vt} if it is not {\tt null}. In the case of a
complete list, {\it vt} is {\tt null} and any values of {\it T} and
{\it vt} will suffice. \todo{While subtle, this bug is potentially
  dangerous and, not addressed by the Smallfoot system.} Allocating a
list, and then addressing only a prefix of the list creates a problem
because the sentinal value could become a pointer in the middle of the
list if the head of the remaining list is freed and memory is
reclaimed and returned in a call to insert into the segment.

We consider {\tt append} function on lists as a representative
example. In Smallfoot, the function is built using a {\tt while} loop,
requires 14 lines, and three annotations: the pre- and post-conditions
and the loop invariant. To maintain a close symmetry between the
definitions, we break the {\it append} function into two pieces, one
for the loop and the other for the main body. The two functions are,
respectively, 4 and 5 lines but require \todo{4 total} lines of
annotations, two for the fixpoint definition \todo{and a framing
  condition on each recursive call}. As with all previous examples,
the proof-automation tactics used for this function apply to all
functions dealing with linked lists.

While the Smallfoot system is faster and requires less verbose
annotations, there are two primary advantages that Ynot has over
it. The first is, as previously mentioned, Ynot functions use
dependent types to represent partial correctness; the Smallfoot system
guarantees only heap-safety. This is captured in the type for {\it
  append}:
\begin{verbatim}
append :: (p q : LinkedList) (lsp lsq : [list A]),
  STsep (lsp ~~ lsq ~~ llist p lsp * llist q lsq)
        (fun r:LinkedList => 
             lsp ~~ lsq ~~ llist r (lsp ++ lsq)).
\end{verbatim}
The second advantage is that of expressivity which has been shown in
previous examples. The ease of use in Smallfoot comes at the price of
restricting programs to a small set of data structures (including
linked lists, doubly linked lists, trees, and xor-lists) for which the
definitions and corresponding inductive properties can be defined
using predicate logic. The theory built for each data structure in
Smallfoot plays the same roll as the theorems for which counts are
given in Figure \ref{loc}. 

\hide{
On the size of proof obligations, the Smallfoot system comes out
ahead, requiring no user-constructed proofs. The drawback to this is
that only inductive data of the following types are allowed: singly
linked lists, doubly linked lists, trees and xor lists. This
restriction is necessary to Smallfoot in order to make the proofs
decidable.

The annotations in the example programs are small mainly because they
use these. While the predicates are hard-coded in Smallfoot,
predicates can be coded directly by the user in Ynot. The style of
Ynot representations are to express data by mapping a ``ghost''
representation into a heap representation. This approach allows
stronger dependent types in the contracts for functions, requireing
the accompanying proof to be a proof of partial correctness rather
than simply of resource constraints. For example, consider the
function to test whether a list is empty in Smallfoot and in Ynot.
\begin{verbatim}
is_empty(r;l) [l |-> t * list(t)] {
  local p;
  p = l->p;
  if (p == NULL) { r = 1; } 
  else { r = 0; }
} [l|-> t * list(t)]
\end{verbatim}

The following function also has the same type:
\begin{verbatim}
is_empty(r;l) [l |-> t * list(t)] {
  r = 1;
} [l|-> t * list(t)]
\end{verbatim}
In Ynot, the \verb|is_empty| function can be defined as follows:
\begin{verbatim}
Definition is_empty A (ll : LinkedList)
  (m : [list A]) :
  STsep (m ~~ rep m ll) 
    (fun res:bool => m ~~ match res with
                            | nil => [res = true]
                            | _   => [res = false]
                          end).
  intros;
  refine (hd <- !ll;
          IfNull hd Then {{Return true}}
          Else {{Return false}}); sep simpl.
Qed.          
\end{verbatim}
The type guarantees that the function is implemented correctly with
respect to the underlying list.

\todo{This is analagous to the implementation of llseg presented in
  the smallfoot paper.}
For example, the following fixpoint expresses the recursive definition
of a linked list segement.  \todo{What is the best way to encode a
  linked list segment?}
\begin{verbatim}
Fixpoint llseg A (hd tl : option ptr) 
  (m : list A) {struct m} :=
  match m with 
    | nil => [hd = tl]
    | a :: b => 
      match hd with 
        | None => [False]
        | Some p => [hd <> tl] * 
            Exists nxt :@ option ptr, 
              hd --> node a nxt * llseg nxt tl b
      end
  end.
\end{verbatim}




Comparison points
\begin{itemize}
\item The use of Coq allows quantifiers in formula. (makes inference
  more difficult but allows more expressive types)
\item Ynot is capble of arbitrary inductive predicates
\item Ynot does not support concurrency which is a strength of
  smallfoot (race detection)
\item Smallfoot is very fast
\item A lot of the ``parallel examples'' don't actually use
  synchronization. These would be pretty trivial to implement in Ynot
  with a function:
\begin{verbatim}
SepParallel (r1 r2 : Type) (P_i Q_i : hprop)
  (P_e : r1 -> hprop) (Q_e : r2 -> hprop) :
     STsep (P_i) (P_e) -> STsep (Q_i) (Q_e)
  -> STsep (P_i * Q_i) (fun r:(r1 * r2) => 
               (P_e (fst r)) * (Q_e (snd r)))
\end{verbatim}
the more interesting aspect of the parallelism comes from the locking
mechanism, which could be encoded in Ynot but only after a little bit
of hacking. (smallfoot uses resource invariants to control this)
\end{itemize}

Smallfoot examples:
\begin{itemize}
\item circular list
\item doubly linked list
\item merge sort
\item ``memory manager'' (basically just a stack)
\item queue
\item tree
\item xor linked list segment
\item xdeq --- uses parallelism (reading and writing buffer)
\end{itemize}
}

\section{Other Related Work}

The ESC/Java~\cite{esc-java} and Spec\#~\cite{spec-sharp} systems tackle some related problems within the classical verification framework.  These systems have strictly less support for modeling data structures than Jahob has, so that it is impractical use them to perform full verifications of many data structures.

A number of systems have been proposed recently to support dependently-typed programming in a setting oriented more towards traditional software development than Coq is.  ATS~\cite{ats} includes novel means for dealing with imperative state, but it includes no proof automation beyond decision procedures for simple base theories like linear arithetic, making it much harder to write verified data structure implementations than in our system.  Concoqtion~\cite{concoqtion} allows the use of Coq for reasoning about segments of general OCaml programs.  While those programs may use imperativity, the Coq reasoning is restricted to pure ``index terms.''  Sage~\cite{sage} supports ``hybrid type-checking,'' where typing invariants may be specified with boolean-valued program functions and checked at runtime.  This approach generally does not enable full static correctness verification.

There is closely related work in the field of shape analysis.  The TVLA system~\cite{tvla} models heap shapes with a first-order logic with a built-in transitive closure operation.  With the right choices of predicates that may appear in inferred specifications, TVLA is able to verify automatically many programs that involve both heap shape reasoning and reasoning in particular decidable theories like arithmetic.

The Xisa system~\cite{xisa} uses an approach similar to ours, as Xisa is based on user specification of inductive characterizations of shape invariants.  Xisa builds this inductive definition mechanism into its framework, while we inherit a more general mechanism from Coq.  Xisa is based on hardcoded algorithms for analyzing inductive definitions and determining when and how they should be unfolded.  Such heuristics lack theoretical guarantees about how broadly they apply.  In the design of our system, we recognize this barrier and allow users not just to define new inductive predicates, but also to extend a generic solver with arbitrary rules for simplifying uses of the predicates.

In comparing the new Ynot environment to the above systems and all others that we are aware of, there are a number of common advantages.  No other system supports both highly-automated proofs based on separation logic (when they work) and highly human-guided proofs (when they are needed), let alone combinations of the two.  None of the systems with significant automation support the combination of imperative and higher-order features, like we handle in the example of our higher-order iterators.  We also find no automated systems that deal with dependent types in programs.  The first of these advantages seems critical in the verification of imperative programs that would be difficult to prove correct even if refactored to be purely functional.  For instance, it seems plausible that our environment could be used eventually to build a verified compiler that uses imperative data structures for efficient dataflow analysis, unification in type inference, and so on.  None of the purely-automated tools that we have surveyed could be applied to that purpose without drastic redesign.  We are not aware of any previous toolkit for manual proof about imperative programs in proof assistants that would make the task manageable; the manual reasoning about state would overwhelm ``the interesting parts'' of compiler verification.



\section{Conclusions \& Future Work}
Irrelevance, concurrency, IO/effects

automation/annotations (loop invariant inference)

Check that bibliography is working~\cite{htt}.

%\bibliographystyle{plainnat}
\bibliographystyle{plain}

\bibliography{bib}

\end{document}
