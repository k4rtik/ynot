\documentclass[preprint,nocopyrightspace]{sigplanconf}
%\usepackage[square, comma, sort&compress]{natbib}
\usepackage{proof}

\usepackage{amsmath}
\usepackage{color}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\hide}[1]{}

\newcommand{\cd}[1]{\texttt{#1}}

\newcommand{\coq}[1]{\mathsf{#1}}
\newcommand{\return}[1]{\coq{return}(#1)}
\newcommand{\bind}{\leftarrow}
\newcommand{\emp}{\mathbf{emp}}
\newcommand{\sep}{\ast}
\newcommand{\pts}[2]{#1 \mapsto #2}
\newcommand{\new}[1]{\coq{new}(#1)}
\newcommand{\free}[1]{\coq{free}(#1)}
\newcommand{\rd}[1]{!#1}
\newcommand{\wri}[2]{#1 := #2}
\newcommand{\himp}{\Rightarrow}

\begin{document}

%\conferenceinfo{PLDI ï¿½09}{ todo }

%\copyrightyear{2005}

%\copyrightdata{1-59593-056-6/05/0006}

\preprintfooter{DRAFT}

%\titlebanner{DRAFT}

\title{Effective Interactive Proofs for Higher-Order Imperative Programs}


\authorinfo{Double blind}
{ }

\maketitle

\begin{abstract}
  We present a new approach for constructing and verifying
  higher-order, imperative programs using the Coq proof assistant.
  Our approach is similar to the shallow embedding of Hoare Type
  Theory that was used in the Ynot project.  However, compared
  to Ynot, our new approach significantly reduces the burden on
  the programmer.  For example, in both systems we have constructed
  fully verified imperative data structures, such as hash-tables with
  higher-order iterators, but the verification burden in the new
  system is reduced by at least an order of magnitude compared to the
  old Ynot system, by replacing manual proof with automation.  The core of
  the automation is a simplification procedure for implications in
  higher-order separation logic, with hooks that allow programmers to
  add domain-specific simplification rules.
  
  Compared to competing approaches to data structure verification, our
  system includes much less code that must be trusted; namely, about a
  hundred lines of Coq code defining a program logic.  All of our
  theorems and decision procedures have or build machine-checkable
  correctness proofs from first principles, removing opportunities for
  tool bugs to create faulty verifications.  We argue for the
  effectiveness of our infrastructure by verifying a number of data
  structures and comparing to similar efforts within other projects.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\terms
%term1, term2

%\keywords
%keyword1, keyword2

\section{Introduction}

A key goal of type systems is to prevent ``bad states'' from arising
in the execution of programs, but today's type systems fail to catch
language-level errors, such as a null-pointer dereference or an
out-of-bounds array index, much less library- and application-specific
errors such as removing an element from an empty queue, failing to
maintain the invariants of a balanced tree, or forgetting to release a
critical resource such as a database connection.  For safety- and
security-critical code, a type system should ideally rule out these
problems and, in the limit, make it possible for programmers to verify
that their code is correct.

There are many recent attempts to extend the scope of type systems to
address a wider range of safety properties.  Representative examples
include ESC/Java~\cite{esc-java}, Spec\#~\cite{spec-sharp},
ATS~\cite{ats}, Concoqtion~\cite{concoqtion}, Sage~\cite{sage}, and
Ynot~\cite{ynot:icfp}.  Each of these systems integrates some form of
specification logic into the type system in order to rule out a wider
range of truly bad states.

However, in the case of ESC/Java, Spec\#, and Sage, the program logic
is too weak to support full verification because these systems rely
completely upon provers to automatically discharge verification conditions.
While there have been great advances in the performance of automated
provers, in practice, they can only handle relatively shallow
fragments of first-order logic.  Thus, programmers are frustrated when
correct code is rejected by the type-checker.  For example,
none of these systems is able to prove that an array index is in
bounds when the constraints step outside quantifier-free
linear arithmetic.

In contrast, ATS, Concoqtion, and Ynot use powerful, higher-order
logics that support a much wider range of policies including (partial)
correctness.  Furthermore, in the case of Ynot, programmers can define
and use connectives in the style of separation logic~\cite{separation}
to achieve simple, modular specifications of higher-order imperative
programs.  For example, a recent paper describes how Ynot was used
to construct fully-verified implementations of data structures such
as queues, hash-tables, and splay-trees, including support for 
iterators.  

The price paid for these more powerful type systems is that, in
general, programmers must provide explicit proofs to convince the
type-checker that code is correct.  Unfortunately, explicit proofs can
be quite large when compared to the code.  For example, in the Ynot
code implementing dequeue for imperative queues, only 7 lines of
program code are required, whereas the proof of correctness is about
70 lines.

This paper reports our experience re-designing and re-implementing
Ynot to dramatically reduce the burden of writing and maintaining the
necessary proofs for full verification.  Like the original Ynot, our
system is based on the ideas of Hoare Type Theory~\cite{htt} and is
realized as an axiomatic extension of the Coq proof
assistant~\cite{CoqArt}.  This allows us to inherit the full power of
Coq's dependent types for writing code, specifications, and proofs,
and it allows us to use Coq's facility for extraction to executable ML
code.  However, unlike the previous version, we have taken advantage
of Coq's tactic language, Ltac, to implement a set of parameterized
semi-decision procedures for automatically discharging, or at least
simplifying, the separation logic-style verification conditions.  The
careful design of these procedures makes it possible for programmers
to teach the prover about new domains as they arise.

We describe this new implementation of Ynot and report on our
experience implementing and verifying various imperative data
structures including stacks, queues, hash tables, binomial trees, and
binary search trees.  When compared with the previous version of Ynot,
we observe roughly an order of magnitude reduction in proof size.  In
most cases, to realize automation, programmers need only prove key
lemmas regarding the abstractions used in their interfaces and plug
these lemmas into our extensible tactics.  More importantly, we show
that the tactics used to generate the proofs are robust to small
changes in the code or specifications.

We also provide a qualitative comparison of our approach to two other
verification projects that have recently appeared in the literature,
Jahob~\cite{jahob} and Smallfoot~\cite{smallfoot}.  To summarize,
Jahob is based on the ideas of ESC/Java and Spec\# but aims for full
verification of imperative data structures.  In general, we found that
it requires many more annotations on the code because of its
``large-footprint'' approach to specifications.  It also requires a
much larger trusted computing base as it combines a number of external
decision procedures, some of which do not generate
separately-checkable proof witnesses justifying their conclusions.
Smallfoot, like Ynot, is based on separation logic and requires many
fewer annotations, but can only verify memory safety and not full
correctness.  Furthermore, Smallfoot only understands a small set of
hard-coded abstractions (including linked list segments and binary trees),
while our new system supports sound extension of the library of
abstractions by programmers.

\section{\label{tutorial}The Ynot Programming Environment}

To a first approximation, Coq can be thought of as a functional
programing language like Haskell or ML, but with support for dependent
types.  For instance, one can have operations with types such as:
\begin{verbatim}
div : nat -> forall n : nat, n <> 0 -> nat
\end{verbatim}
which uses dependency to capture the fact that the function can only
be called when a proof can be supplied that the second argument is
non-zero.  One can also write functions such as:
\begin{verbatim}
Definition avg (x:list nat) : nat :=
  let sum := fold plus 0 x in
  let len := length x in
  match eq_nat_dec len 0 with
    | inl(pf1: len = 0) => 0
    | inr(pf2: len <> 0) => div sum len pf2
  end.
\end{verbatim}

This function averages the values in a list of natural numbers.  It has a normal type like you might find in ML, and its implementation begins in an ML-like way, using a higher-order \cd{fold} function.  The interesting part is the \cd{match} expression.  We match on the result of a call to \cd{eq\_nat\_dec}, a dependently-typed natural number comparison function.  This function returns a sum type with an equality proof in one branch and an inequality proof in the other. We bind a name for each proof explicitly in the pattern for each \cd{match} case.  The proof that \cd{len} is not zero is passed to \cd{div} to justify the safety of the operation.

All Coq functions have to be pure -- terminating without side effects.  This is necessary to ensure that proofs really are proofs, with no spurious ``proofs by infinite loop.''  Ynot extends Coq with support for side-effecting computations.  Similar to Haskell, we introduce a monadic type constructor \cd{ST T} which describes computations that might diverge and that might have side effects, but that, if they do return, return values of type \cd{T}.  We can use \cd{ST} to safely keep the ``effectful'' computations separate from the pure computations.

Unlike Haskell's \cd{IO} monad, the \cd{ST} type is parameterized by a pre- and post- condition, which can be used to describe the effects of the computation on a mutable store.  Alternatively, one can think of the axiomatic base of Ynot as a fairly standard Hoare logic.  The main difference of our logic from usual presentations is that it is designed to integrate well with Coq's functional programming language, so that we formalize a language of expressions instead of commands, hence the IO-monad-like presentation.  A program derivation is of the form $\{P\} \; e \: \{Q\}$, where $P$ is a precondition predicate over heaps, and $Q$ is a postcondition predicate over initial heaps, functional values of $e$, and final heaps.  For instance, we can derive
$$\{\lambda \_. \; \top\} \; \return{1} \; \{\lambda h, v, h'. \; h' = h \land v = 1\}$$
and
$$\begin{array}{c}
  \{\lambda h. \coq{sel}(h, p_1) = p_2\} \\
  x \bind \; \rd{p_1}; \wri{x}{1} \\
  \{\lambda h, \_, h'. \; \coq{sel}(h, p_1) = p_2 \land h' = \coq{upd}(h, p_2, 1)\}
\end{array}$$

Unlike other systems, Ynot does not distinguish between programs and derivations. Rather, the two are combined into one dependent type family, whose indices give the specification of a program.  For instance, the type of the return example would be:
\begin{verbatim}
ST (fun _ => True) (fun h v h' => h' = h /\ v = 1)
\end{verbatim}

Heaps are represented as functions from pointers to dynamically-typed packages, which are easy to implement in Coq with an inductive type definition.  The pointer read rule enforces that the heap value being read has the type that the code expects.  The original Ynot paper~\cite{ynot:icfp} contains further details of the base program logic.


\subsection{A Derived Separation Logic}

\begin{figure*}
  $$\infer{\{\emp\} \; \return{v} \; \{\lambda v'. \; [v = v']\}}{}
  \quad \infer{\{P_1\} \; x \bind e_1; e_2 \; \{Q_2\}}{
    \{P_1\} \; e_1 \; \{Q_1\}
    & (\forall x, \{P_2(x)\} \; e_2 \; \{Q_2\})
    & (\forall x, Q_1(x) \himp P_2(x))
  }$$

  $$\infer{\{\emp\} \; \new{v} \; \{\lambda p. \; \pts{p}{v}\}}{}
  \quad \infer{\{\exists v, \pts{p}{v}\} \; \free{p} \; \{\lambda \_. \; \emp\}}{}$$

  $$\infer{\{\exists v, \pts{p}{v} \sep P(v)\} \; \rd{p} \; \{\lambda v. \; \pts{p}{v} \sep P(v)\}}{}
  \quad \infer{\{\exists v, \pts{p}{v}\} \; \wri{p}{v'} \; \{\lambda \_. \; \pts{p}{v'}\}}{}$$

  $$\infer{\{P\} \; e \; \{Q\}}{
    P \himp P'
    & \{P'\} \; e \; \{Q'\}
    & Q' \himp Q
  }
  \quad \infer{\{P \sep R\} \; e \; \{Q \sep R\}}{
    \{P\} \; e \; \{Q\}
  }$$

  \caption{\label{STsep}The main rules of the derived separation logic}
\end{figure*}

Direct reasoning about heaps leads to very cumbersome proof obligations, with many sub-proofs that pairs of pointers are not equal.  Separation logic~\cite{separation} is the standard tool for reducing that complexity.  The previous Ynot system built a separation logic on top of the axiomatic foundation, and we do the same here.  We introduce no new syntactic classes of separation logic formulas.  Instead, we define functions that operate on arbitrary predicates over heaps, with the intention that we will only apply them on separation-style formulas.  Nonetheless, it can be helpful to think of our ``assertion language'' as defined by:
$$\begin{array}{rcl}
  P &::=& [p] \mid \emp \mid \pts{x}{y} \mid P \sep P \mid \exists x, P
\end{array}$$

For any pure Coq proposition $p$, $[p]$ is the heap predicate that asserts that $p$ is true and the heap is empty.  $\emp$ asserts that the heap is empty, and $\pts{x}{y}$ asserts that the heap contains only a mapping from $x$ to $y$.  $P_1 \sep P_2$ asserts that the heap can be broken into two heaps $h_1$ and $h_2$ with disjoint domains, such that $h_1$ satisfies $P_1$ and $h_2$ satisfies $P_2$.  The final clause provides existential quantification.

It is worth pointing out that we simplify the assertion language substantially by taking advantage of Coq's base language.  We do not need to include program variables in the logic, because any Coq variable may be included anywhere in a Coq development, including within one of our assertions.  The same argument allows us to avoid explicit mention of specification variables, sometimes also called ``ghost state'' variables.

The embedding in Coq provides much more expressive formulas than in most systems based on separation logic.  Not only can any pure proposition be injected with $[\cdot]$, but we can also use arbitrary Coq computation to build impure assertions.  For instance, we can include calls to custom recursive functions that return assertions.  We need no special support in the assertion language to accommodate this, and Coq's theorem-proving support for reasoning about pattern-matching recursive functions can be used without modification.

Perhaps surprisingly, we have general success in implementing realistic examples using just these connectives.  Standard uses of other connectives can often be replaced by uses of higher-order features, and the connectives that we \emph{do} use are particularly amenable to automation.  Fully-automated systems like Smallfoot build in similar restrictions, but it surprised us that we needed little more to do functional verification.

\medskip

What we have described so far is the same as in the original Ynot
work.  The big departure of our new system is that we define a more
standard separation logic.  The old Ynot separation logic used
``binary postcondtions'' that may refer to both the initial and final
heaps.  This is in stark contrast to traditional separation logics,
where all assertions are separation formulas over a single heap, and all verification proof obligations are implications between such assertions.  The utility of this formalism has been born out in the wealth of tools that have used separation logic for automated verification.  In contrast, proofs for the binary postconditions in the old Ynot tended to involve at least tens of steps of manual proof per line of program code.

Why did the original Ynot use this nonstandard program logic?  The answer has to do with the need for an effective analogue of specification variables.  In traditional separation logic, specification variables are commonly used to relate pre- and post-conditions ensure that parts of state are preserved by commands, when the same specification variable appears in both the precondition and postcondition of a derivation.  In contrast, the old Ynot used binary postconditions for the same purpose, asserting equations between parts of the pre and post heaps.

The final output of a Ynot program is an executable piece of OCaml or Haskell code, produced with Coq's program extraction facility.  We can use standard Coq variables as specification variables, but, by default, they will remain at runtime, reducing the efficiency of programs.  One of the main innovations of the new work we present involves a new way of encoding specification variables in Coq, such that they will be erased by the standard program extraction facility.  This makes it possible to use a more standard separation logic, with the corresponding decrease in the difficulty of automating proofs.  We will present this new technique by example in later subsections.

Figure \ref{STsep} presents the main rules of our separation logic.  The notable divergence from common formulations is in the use of existential quantifiers in the rules for freeing, reading, and writing.  These differences make sense because Ynot is implemented within a constructive logic.  A more standard, ``classical'' separation logic would, for instance, require that, in the rule for $\coq{free}$, the value $v$ pointed to by $p$ be provided as an argument to the proof rule.  In constructive logic, such a value can only be produced when it can be computed by an algorithm.  Not only that, but we would not be able to use any facts implied by the current heap assertion to build one of these witnesses, and perhaps the witness can only be proved to exist using such facts.  The explicit existential quantifier frees us to reason ``inside the assertion language'' in finding the witness.

One consequence is that the ``read'' rule must take a kind of explicit framing condition.  This condition is parameterized by the value being read from the heap, making it a kind of description of the ``neighborhood around that value'' in the heap.  More standard separation logics force the exact value being read to be presented as an argument to the proof rule, but here we want to allow verification of programs where the exact value to read cannot be computed from the pieces of pure data that are in scope.

\medskip

In the rest of this section, we will introduce the Ynot programming environment more concretely, via several examples.


\subsection{Verifying an Implementation of Imperative Stacks}

\begin{figure}
  \begin{verbatim}
Module Type STACK.
  Parameter t : Set -> Set.
  Parameter rep T : t T -> list T -> hprop.

  Parameter new T :
    STsep __ (fun s : t T => rep s nil).
  Parameter free T (s : t T) :
    STsep (rep s nil) (fun _ : unit => __).

  Parameter push T (s : t T) (x : T) (ls : [list T]) :
    STsep (ls ~~ rep s ls)
          (fun _ : unit => ls ~~ rep s (x :: ls)).
  Parameter pop T (s : t T) (ls : [list T]) :
    STsep (ls ~~ rep s ls) (fun xo : option T => ls ~~
            match xo with
              | None => [ls = nil] * rep s ls
              | Some x => Exists ls' :@ list T,
                            [ls = x :: ls'] * rep s ls'
            end).
End STACK.
  \end{verbatim}

  \caption{\label{Stack}The signature of an imperative stack module}
\end{figure}

Figure \ref{Stack} shows the signature of a Ynot implementation of the
stack ADT.  The signature is expressed in Coq's ML-like module system.
Each implementation contains a type family \cd{t}, where, for any
type \cd{T}, a value of \cd{t(T)} represents a stack storing elements
of \cd{T}.  The \cd{rep} component of the interface relates an
imperative stack \cd{s} to a functional list \cd{ls} in a
particular state.  Thus, \cd{rep s ls} is a predicate on heaps
(\cd{hprop}) which can be read as ``\cd{s} represents the list \cd{ls}'' in the
current state.  Just as abstraction over the type family \cd{t} allows
an implementation to choose different data structures to encode the
stack, abstraction over the assertion \cd{rep} allows an
implementation to choose different invariants connecting the concrete
representation to an idealized model.  

The type of the \cd{new} operation tells us that it expects an empty heap on input, and on output the heap contains just whatever mappings are needed to satisfy the representation invariant between the function return value and the empty list.  The ASCII notation \cd{\_\_} stands for the $\emp$ of usual pencil-and-paper separation logics.  The \cd{free} operation takes a stack \cd{s} as an argument, and it expects the heap to satisfy \cd{rep} on \cd{s} and the empty list.  The post state shows that all heap values associated with \cd{s} are freed.

The specification for \cd{push} says that it expects any valid stack as input and modifies the heap so that the same stack that stood for some list \cd{l} beforehand now stands for the list \cd{x :: l}, where \cd{x} is the appropriate function argument.  We see an argument \cd{ls} with type \cd{[list T]}.  The brackets are a notation defined by the Ynot library, standing for \emph{computational irrelevance}.  That is, the type-checker should enforce that the value of \cd{ls} is not needed to execute the function.  Rather, such values may only be used in stating specifications and discharging proof obligations.  In other words, irrelevant variables act just like specification variables, but we do not need to build any special support for them into our assertion language.  We use Coq's notation scope mechanism to overload brackets for writing irrelevant types and lifted pure propositions.

For an assertion \cd{P} that mentions the irrelevant variable \cd{x}, the notation \cd{x \textasciitilde\textasciitilde \; P} must be used to explicitly ``unpack'' \cd{x}.  The type of the unpack operation is such that it may only be applied to assertions and may not be used to allow an irrelevant variable's value to ``leak into'' the computational part of a program.

The type of \cd{pop} showcases how we avoid the disjunctive connectives of separation logic.  The function returns an optional \cd{T} value, which will be \cd{None} when the stack is empty and will be \cd{Some x} when \cd{x} is at the top of the stack.  We use a Coq \cd{match} expression to give a different postcondition for each case.

\medskip

We can implement a module satisfying this signature.  With the type \cd{T} as a local variable, we can define the type of nodes of the linked lists that we will use.

\begin{verbatim}
Record node : Set := Node {
  data : T;
  next : option ptr
}.
\end{verbatim}

To define the representation invariant, we want a recursive function specifying what it means for a possibly-null pointer to represent a functional list.

\begin{verbatim}
Fixpoint listRep (ls : list T) (hd : option ptr)
    {struct ls} : hprop :=
  match ls with
    | nil => [hd = None]
    | h :: t => match hd with
                  | None => [False]
                  | Some hd => Exists p :@ option ptr,
                      hd --> Node h p * listRep t p
                end
  end.
\end{verbatim}

We can represent stacks as untyped pointers to the heads of linked lists built from \cd{Node}s.

\begin{verbatim}
Definition stack := ptr.
\end{verbatim}

We achieve type safety through the representation invariant.

\begin{verbatim}
Definition rep (s : stack) (ls : list T) : hprop :=
  Exists po :@ option ptr, s --> po * listRep ls po.
\end{verbatim}

Before we start implementing the ADT methods, we should set up some proof automation machinery.  Systems like Smallfoot~\cite{smallfoot} have hardcoded support for particular heap predicates like acyclic linked list-ness, cyclic linked list-ness, and so on.  These systems perform simplifications on formulas that mention the predicates that they understand.  In contrast, with Ynot, the programmer can define his own new predicates, as we have just done.  Not only that, but he can also prove lemmas that correspond with the simplification rules built into automated tools, and he can plug his lemmas into a general separation logic solver.  All of this is done with no risk that a mistake by the programmer will lead to a faulty verification; every lemma must be proved from first principles.

For the current example, we need two lemmas for ``unfolding'' the definition of \cd{listRep} at points where we know for sure whether or not the pointer argument is null.

\begin{verbatim}
Theorem listRep_None : forall ls,
  listRep ls None ==> [ls = nil].
  destruct ls; sep fail idtac.
Qed.

Theorem listRep_Some : forall ls hd,
  listRep ls (Some hd) ==> Exists h :@ T,
    Exists t :@ list T, Exists p :@ option ptr,
      [ls = h :: t] * hd --> Node h p * listRep t p.
  destruct ls; sep fail ltac:(try discriminate).
Qed.
\end{verbatim}

The proofs are given in Coq's tactic language.  We ask to do a case analysis on the structure of the list \cd{ls}, applying the parametrized separation logic solver in each case.  We will go into more detail shortly on the significance of the two arguments to the \cd{sep} procedure.

Now we can plug our two unfolding rules into the separation solver.  We define a local procedure for simplifying separation implications.

\begin{verbatim}
Ltac simp_prem := simpl_IfNull;
  simpl_prem ltac:(apply listRep_None
                   || apply listRep_Some).
\end{verbatim}

Our procedure, or \emph{tactic} in Coq parlance, first calls a simplification procedure associated with a syntax extension for checking pointer nullness.  Next, our procedure calls a tactic \cd{simpl\_prem} from the Ynot library, for simplifying premises of implications.  The argument to \cd{simpl\_prem} gives a procedure to attempt on each premise, until no further progress can be made.

We can plug our domain-specific simplification tactic into the generic separation logic solver.  We define a new tactic that will suffice to prove all goals that will arise in this example.  The first argument to \cd{sep} gives a procedure to apply to simplify premises before starting the main proof search, and the second argument gives a procedure to attempt on individual subgoals throughout proof search.  The \cd{discriminate} tactic solves goals whose premises include inconsistent equalities over values of datatypes, like \cd{nil = x :: ls}; and adding \cd{try} in front prevents \cd{discriminate} from signaling an error if no such equality exists.

\begin{verbatim}
Ltac t := unfold rep; sep simp_prem
                          ltac:(try discriminate).
\end{verbatim}

We implement each method by stating its type as a proof search goal, using tactics to realize the goal step by step.  Here is the implementation of the \cd{new} operation.

\begin{verbatim}
Definition new : STsep __ (fun s => rep s nil).
  refine {{New (@None ptr)}}; t.
Qed.
\end{verbatim}

A simple two-step proof script suffices.  We first use the \cd{refine} tactic to provide a ``template'' for the implementation.  The template may have holes in it, and each hole is added as a subgoal.  We chain our \cd{t} tactic with the semicolon operator, so that \cd{t} is applied to each subgoal generated from a hole.  This is enough to finish the implementation and proof.

The holes in the refinement are not apparent from the syntax we have used.  We apply Coq's syntax extension system to hide such details for most programs.  In this case, it is the \cd{\{\{...\}\}} syntax that hides the holes.  This syntax requests simultaneous strengthening and weakening.  It expands to an explicit call to a rule that takes two proofs as arguments.  Those arguments are filled in as holes by the syntax extension.

The definitions of \cd{free} and \cd{push} are not much more complicated.

\begin{verbatim}
Definition free s : STsep (rep s nil)
    (fun _ : unit => __).
  intros; refine {{Free s}}; t.
Qed.
\end{verbatim}

\begin{verbatim}
Definition push s x ls : STsep (ls ~~ rep s ls)
    (fun _ : unit => ls ~~ rep s (x :: ls)).
  intros; refine (hd <- !s;
    nd <- New (Node x hd);
    {{s ::= Some nd}}
  ); t.
Qed.
\end{verbatim}

The implementation of \cd{pop} uses another syntax extension, which provides an \cd{IfNull} expression form.  The \cd{option}-typed argument to \cd{IfNull} is checked for nullness (i.e., equality to \cd{None}).  In an \cd{Else} branch, where the pointer is known to be non-null, that fact is added as a usable proof hypothesis, and the variable being tested is rebound with a non-\cd{option} type.

\begin{verbatim}
Definition pop s ls :
  STsep (ls ~~ rep s ls) (fun xo => ls ~~
          match xo with
            | None => [ls = nil] * rep s ls
            | Some x => Exists ls' :@ list T,
                          [ls = x :: ls'] * rep s ls'
          end).
  intros; refine (hd <- !s;
    IfNull hd Then
      {{Return None}}
    Else
      nd <- !hd;
      Free hd;;
      s ::= next nd;;
      {{Return (Some (data nd))}}); t.
Qed.
\end{verbatim}

We complete the implementation with a trivial definition of the type family \cd{t}, relying on the representation invariant to ensure proper use.

\begin{verbatim}
Definition t (_ : Set) := stack.
\end{verbatim}

\begin{figure}
  \begin{verbatim}
module Stack = 
 struct 
  type 't node = { data : 't; next : ptr option }
  let data n = n.data
  let next n = n.next
  type stack = ptr
  
  let coq_new =
    sepWeaken (sepStrengthen (sepFrame (sepNew None)))
  
  let free s =
    sepWeaken (sepStrengthen (sepFrame (sepFree s)))
  
  let push s x =
    sepBind (sepStrengthen (sepRead s)) (fun hd ->
      sepBind (sepStrengthen (sepFrame
          (sepNew { data = x; next = hd })))
        (fun nd ->
        sepWeaken (sepStrengthen (sepFrame
          (sepWrite s (Some nd))))))
  
  let pop s =
    sepBind (sepStrengthen (sepRead s)) (fun hd ->
      match hd with
        | Some v ->
            sepBind (sepStrengthen (sepRead v)) (fun nd ->
              sepSeq (sepStrengthen (sepFrame (sepFree v)))
                (sepSeq (sepStrengthen (sepFrame
                    (sepWrite s (next nd))))
                  (sepWeaken
                    (sepStrengthen (sepFrame
                      (sepReturn (Some (data nd))))))))
        | None -> sepWeaken (sepStrengthen (sepFrame
                    (sepReturn None))))
  
  type 'x t = stack
 end
  \end{verbatim}

  \caption{\label{extracted}OCaml code extracted from the stack example}
\end{figure}

For our modest efforts, we can now extract an executable OCaml version of our module.  Figure \ref{extracted} shows the result of running Coq's automatic extraction command on our \cd{Stack} module.

In the method implementations, we see invocations of functions whose names begin with \cd{sep}.  These come from the Ynot library, and we must provide their OCaml implementations.  Any Ynot program that returns a type \cd{t} may be represented in \cd{unit -> t} in OCaml, regardless of the specification appearing in the original Coq type.  This makes it easy to implement the basic functions, in the spirit of how the Haskell IO monad is implemented.  We see calls to explicit weakening, strengthening, and framing rules in the extracted code.  In OCaml, these can be implemented as no-ops and elided by an optimizer.

Notice that all specification variables and proofs are eliminated automatically by the Coq extractor.  With the elision of weakening and related operations, we arrive at exactly the kind of monadic code that is standard fare for Haskell, such that the body of optimizations developed for Haskell can be put to immediate use in creating an efficient compilation pipeline for Ynot.


\subsection{Verifying Imperative Queues}

It is not much harder to implement and verify a queue structure.  We define an alternate list representation, parameterized by head and tail pointers.

\begin{verbatim}
Fixpoint listRep (ls : list T) (hd tl : ptr)
    {struct ls} : hprop :=
  match ls with
    | nil => [hd = tl]
    | h :: t => Exists p :@ ptr, hd --> Node h (Some p)
                  * listRep t p tl
  end.

Record queue : Set := Queue {
  front : ptr;
  back : ptr
}.

Definition rep' (ls : list T) (fr ba : option ptr) :=
  match fr, ba with
    | None, None => [ls = nil]
    | Some fr, Some ba => Exists ls' :@ list T,
        Exists x :@ T, [ls = ls' ++ x :: nil]
          * listRep ls' fr ba * ba --> Node x None
    | _, _ => [False]
  end.
          
Definition rep (q : queue) (ls : list T) :=
  Exists fr :@ option ptr, Exists ba :@ option ptr,
    front q --> fr * back q --> ba * rep' ls fr ba.
\end{verbatim}

For this representation, we prove similar ``unfolding'' lemmas to those we proved for stacks, with comparable effort.  We also need a new lemma for ``unfolding a queue from the back.''

\begin{verbatim}
Lemma rep'_back : forall ls fr ba,
  rep' ls (Some fr) ba
  ==> Exists nd :@ node, fr --> nd
    * Exists ls' :@ list T, [ls = data nd :: ls']
      * match next nd with
          | None => [ls' = nil]
          | Some fr' => rep' ls' (Some fr') ba
        end.
\end{verbatim}

The proof of the lemma relies on some lemmas about pure functional lists.  With those available, we prove \cd{rep'\_back} in under 20 lines.  When we plug this and the two other unfolding lemmas into the \cd{sep} procedure, we arrive at quite a robust decision procedure for separation assertions about lists that may be modified at either end.

Again, every proof obligation for our queue implementation is proved by a \cd{t} tactic built from \cd{sep}.  We write under 10 lines of new tactic hints to be applied during proof search, and we must prove one key lemma by induction.

\begin{verbatim}
Lemma push_listRep : forall ba x nd ls fr,
  ba --> Node x (Some nd) * listRep ls fr ba
  ==> listRep (ls ++ x :: nil) fr nd.
  Hint Resolve himp_comm_prem.
  induction ls; t.
Qed.
\end{verbatim}

We suggest a hint based on commutativity of separating conjunction, state the inductive structure of the proof, and discharge the two cases with our specialized \cd{t}.  This is an instance of a common pattern, where lemmas requiring inductive proofs must be stated explicitly, while most other separation facts are proved automatically by \cd{sep}.


\subsection{Arrays}

It is also easy to support arrays within our framework.  We can define suitable constants in the base program logic:

\begin{verbatim}
Parameter array : Set.
Parameter array_length : array -> nat.  
Parameter array_plus : array -> nat -> [ptr].
\end{verbatim}

The \cd{array\_plus} function is for computing a pointer to a particular cell of an array.  Note how the type of this function forces the resulting pointer to be treated in a computationally irrelevant way.  That is, it is easy to reason about the separate cells of an array with the separation logic connectives we already introduced, but client code cannot actually calculate pointers from arrays ``at runtime.''  This makes it possible to represent Ynot arrays with OCaml arrays.

With an infix notation \cd{@+} defined for \cd{array\_plus}, it is easy to write a type for array update.  We use the notation \cd{x :\textasciitilde\textasciitilde \; v in P} to bind \cd{x} to the hidden value of specification value \cd{v} within the assertion \cd{P}.  We use the shorthand \cd{p -->?} to indicate that \cd{p} points to some unknown value of unknown type.

\begin{verbatim}
Parameter upd_array (A:Type)(a:array)(i:nat)(v:A) :
  STsep (p :~~ a @+ i in p -->?)
        (fun _ : unit =>
               p :~~ a @+ i in p --> v).
\end{verbatim}

The type of the array allocation operation indicates that every pointer within the array is initialized to some unknown value.  The notation \cd{\{@ e | i <- min + len\}} denotes an iterated separating conjunction where the variable \cd{i} ranges from \cd{min} to \cd{min + len - 1}.

\begin{verbatim}
Parameter new_array (n:nat) :
  STsep __ (fun a : array => [array_length a = n]
    * {@ p :~~ a @+ i in p -->? | i <- 0 + n}.
\end{verbatim}


\subsection{Loops}

Like with many semi-automated verification systems, we require annotations that are equivalent to ``loop invariants.''  Since Coq's programming language is functional, it is more natural to write ``loops'' as recursive functions, and the ``loop invariants'' become the pre- and postconditions of these functions.

We support general recursion with a primitive fixpoint operator in the base program logic, and it is easy to build a separation logic version on top of that.  We can also build multiple-argument recursive function forms on top of the single-argument form.

An example is a \cd{getElements} function, defined in terms of the list invariant that we wrote for the stack example.  This operation returns the functional equivalent of an imperative list.  The task is not so trivial as it may look at first, because the computational irrelevance of the function's second argument prohibits its use to influence the return value.

\begin{verbatim}
Definition getElements (hd : option ptr) (ls : [list A]) :
  STsep (ls ~~ listRep ls hd)
        (fun res : list A => ls ~~ [res = ls]
          * listRep ls hd).
  refine (Fix2
    (fun hd ls => ls ~~ listRep ls hd)
    (fun hd ls res => ls ~~ [res = ls] * listRep ls hd)
    (fun self hd ls => 
      IfNull hd Then
        {{Return nil}}
      Else
        fn <- !hd;
        rest <- self (next fn) (ls ~~~ tail ls) <@> _;
        {{Return (data fn :: rest)}})); t.
Qed.
\end{verbatim}

The code demonstrates a use of one of the derived fixpoint combinators, \cd{Fix2}.  Of the three arguments that we pass, the first two give the precondition and postcondition in terms of the two ``real'' arguments (and, for the postcondition, the return value).  The third argument is the function body.  It takes a recursive self-reference as its first argument, followed by the two ``real'' arguments.

The notation \cd{x \textasciitilde\textasciitilde\textasciitilde \; e} is for building a new computationally-irrelevant value out of an old one.  The notation \cd{e <@> P} is explicit invocation of the frame rule.  With the current system, one usually wants to invoke that rule at each function call.  The framing assertion can be written as an underscore to ask that it be inferred.  We would like to avoid such explicit framings, and they only remain because it is unclear how to implement our syntax extensions in a way that avoids them.


\subsection{A Dependently-Typed Memoizing Function}

\begin{figure}
  \begin{verbatim}
Module Type MEMO.
  Parameter T : Set.
  Parameter t : forall (T' : T -> Set),
    hprop
    -> (forall x, T' x -> Prop)
    -> Set.
  Parameter rep : forall (T' : T -> Set)
    (inv : hprop) (fpost : forall x, T' x -> Prop),
    t inv fpost -> hprop.
  Parameter create : forall (T' : T -> Set)
    (inv : hprop) (fpost : forall x, T' x -> Prop),
    (forall x, STsep inv
      (fun y : T' x => [fpost _ y] * inv))
    -> STsep __ (fun m : t inv fpost => rep m).
  Parameter funcOf : forall (T' : T -> Set)
    (inv : hprop) (fpost : forall x, T' x -> Prop)
    (m : t inv fpost),
    forall (x : T), STsep (rep m * inv)
      (fun y : T' x => rep m * [fpost _ y] * inv).
End MEMO.
  \end{verbatim}
  \caption{\label{memoize}Signature of a memoization module}
\end{figure}

As far as we have been able to determine, all previous tools for data structure verification lack either aggressive automation or support for higher-order features.  The original Ynot supported easy integration of higher-order functions and dependent types, but the very manual proof style became even more onerous for such uses.  Our reimplemented Ynot maintains the original's higher-order features, and our proof automation integrates very naturally with them.  This is a defining advantage of our new framework over all alternatives.

For instance, it is easy to define a module supporting memoization of imperative functions.  Figure \ref{memoize} gives the signature of our implementation, which is actually an ML-style functor that produces implementations of this signature when passed appropriate input modules.  The type \cd{T} is the domain of memoizable functions, and types like \cd{t inv fpost} stand for memo tables.  The argument \cd{inv} is an assertion giving an invariant that the memoized function maintains, and the pure assertion \cd{fpost} gives a relation between inputs and outputs of the function.  The \cd{rep} predicate captures the heap invariants associated with a memo table.  The \cd{create} function produces a memo table when passed an imperative function with the proper specification.  Finally, the \cd{funcOf} function maps a memo table to a function that consults the table to avoid recomputation.

We can implement a \cd{MEMO} functor in 50 lines when we use a memo table that only caches the most recent input-output pair.  Like in the previous examples, we build a specialized automation procedure with a one-line instantiation of library tactics.  We give a 7-line definition of \cd{rep}, give one one-liner proof of a lemma to use in proof search, and include two lines of annotations within the definition of \cd{funcOf}.  All of the rest of the development is no longer or more complicated than in ML.  Compared to ML, we have the great benefit of using types to control the behavior of functions to be memoized.  A function could easily thwart an ML memoizer by producing unexpected computational effects.

  
\section{Tactic Support}

The examples from the last section show how much of the gory details of proofs can be hidden from programmers.  In actuality, every command triggers the addition of one or more proof obligations that cannot be discharged effectively by any of the built-in Coq automation tactics.  Not only is it hard to prove the obligations, it is also hard to infer the right intermediate specifications.  Our separation logic formulas range well outside the propositional fragment that automated tools tend to handle; specification inference and proving must deal with higher-order features.

Here is an example of the proof obligations generated for the code we gave earlier for the stack \cd{push} method.  Numbers prefixed with question marks are unification variables, whose values the \cd{sep} tactic must infer.

\begin{verbatim}
 ls ~~ rep s ls ==>
 Exists v :@ option ptr, s --> v * ?200 v

 forall v : option ptr, s --> v * ?200 v ==> ?192 v

 ?192 hd ==> ?217 * __

 forall v : ptr, ?217 * v --> Node x hd ==> ?206 v

 ?206 nd ==> ?234 * (Exists v' :@ ?231, s --> v')

 ?234 * s --> Some nd ==> ls ~~ rep s (x :: ls)
\end{verbatim}

We can see that each goal has at most one new unification variable standing for a specification, and each such variable has its value determined by the value of the variable from the previous goal.  This is no accident; we designed our combinators and notations to have this property.

Now we are ready to describe what the \cd{sep} tactic does.  We do not have space to include the literal Coq code implementing it; we will give pseudocode instead.  The implementation is in Coq's Ltac language~\cite{Ltac}, a domain-specific, dynamically-typed language for writing proof-generating proof search procedures.  Here is an example Ltac program that implements simplifications of goals that contain conjunctions.

\begin{verbatim}
Ltac un_and :=
  repeat match goal with
           | [ H : _ /\ _ |- _ ] => destruct H
           | [ |- _ /\ _ ] => split
         end.
\end{verbatim}

This program repeats a pattern-matching process until it fails.  Each iteration looks first for a hypothesis that states a conjunction.  If one is found, the tactic \cd{destruct} is used to split it into two new hypotheses.  If no conjunctive hypotheses remain, the second \cd{match} case checks if the conclusion is a conjunction.  If so, the \cd{split} tactic is used to split the goal into one subgoal for each conjunct.

\medskip

As we have seen, \cd{sep} takes two arguments, which we will call \cd{unfolder} and \cd{solver}.  The task of \cd{unfolder} is to simplify goals before specification inference, usually by unfolding definitions of recursive predicates, based on known facts about their arguments.  The task of \cd{solver} is to solve all of the goals that remain after generic separation logic reasoning is applied.

Coq comes with the standard tactic \cd{tauto}, for proving propositional tautologies.  There is a more general version of \cd{tauto} called \cd{intuition}, which will apply a user-supplied tactic to finish off sub-proofs, while taking responsibility for handling propositional structure on its own.  The \cd{intuition} tactic also has the helpful property of leaving for the user any subgoals that it could not establish.  \cd{sep} is meant to be an analogue of \cd{intuition} for separation logic.  We also want it to handle easy instantiation of existential quantifiers, since they appear so often in our specifications.

We can divide the operation of \cd{sep} into five main phases.  We will sketch the workings of each phase separately.

\subsection{Simple Constraint Solving}

It is trivial to determine the proper value for any unification variable appearing alone on one side of the implication.  For instance, given the goal
\begin{verbatim}
p --> x * q --> y ==> ?123
\end{verbatim}
we simply set \cd{?123} to \cd{p --> x * q --> y}.  Given the slightly more complicated goal
\begin{verbatim}
p --> x * q --> y ==> ?123 x
\end{verbatim}
we abstract the lefthand side to produce\cd{fun x' => p --> x' * q --> y}.

\subsection{Intermediate Constraint Solving}

When the trivial unification rules are not sufficient, we need to do more work.  We introduce names for all existential quantifiers and computationally-irrelevant variables in the premise.  For instance, starting with
\begin{verbatim}
m ~~ Exists v :@ T, p --> v * rep m v
==> ?123 * Exists x :@ T, p --> x
\end{verbatim}
we introduce names to simplify the premise, leading to this goal:
\begin{verbatim}
p --> v' * rep m' v' ==> ?123 * Exists x :@ T, p --> x
\end{verbatim}
Now we run the user's \cd{unfolder} tactic, which might simplify some use of a definition.  Let us assume that no such simplification occurs for this example.  We notice that the points-to fact on the right mentions the same pointer as a fact on the left, so these two facts may be unified, implying \cd{x = v'}.  Canceling this known information, we are left with
\begin{verbatim}
rep m' v' ==> ?123
\end{verbatim}
which is resolvable almost trivially.  We cannot give \cd{?123} a value that mentions the variables \cd{m'} and \cd{v'}, since we introduced them locally, placing them out of the scope where the specification is needed.  Instead, we remember how each local variable was introduced and ``re-quantify'' at the end, like this:
\begin{verbatim}
m ~~ Exists v :@ T, rep m v ==> ?123
\end{verbatim}
Now the trivial unification is valid.  The crucial part of this process was the matching of the two points-two facts.  We have special-case rules for matching conclusion facts under quantifiers, for conclusions that match the preconditions of the read, write, and free rules.  Beyond that, we apply cancelation of identical terms on the two sides of the implication, when those terms do not fall under the scope of quantifiers.  These simple rules seem to serve well in practice.

\subsection{Premise Simplification}

After specification inference, the next step is to simplify the premise of the implication.  Any $\emp$ in the premise may be removed, and any lifted pure formula $[p]$ may be removed from the implication and added instead to the normal proof context.  We also remove existential quantifiers and irrelevant variable unpackings in the same way as in the previous phase.

\subsection{Conclusion Simplification}

The main \cd{sep} loop is focused on dealing with parts of the conclusion.  We remove occurrences of $\emp$, and we remove any pure formula $[p]$ that the user's \cd{solver} tactic is able to prove.  An existential formula \cd{Exists x :@ T, P(x)} in the conclusion is replaced by \cd{P(?456)}, for a fresh unification variable \cd{?456}.  When no more of these rules apply, we look for a pair of unifiable subformulas on the sides of the implication.  All such pairs are unified and crossed out.  This may determine the value of a variable introduced for an existential quantifier.  For instance, if we started with \cd{Exists x :@ T, p --> x} on the righthand side and replaced that formula with \cd{p --> ?456}, then if a formula \cd{p --> y} appears on the lefthand side, we would unify these new formulas, deducing that \cd{?456} must be \cd{y}.

% \begin{enumerate}
% \item Look for an easy case of specification inference.
%   \begin{enumerate}
%   \item If a unification variable $U$ appears alone in the implication conclusion, set $U$ equal to the premise.
%   \item If the conclusion is a unification variable $U$ applied to a program variable $x$, set $U$ equal to a function that abstracts over the occurrences of $x$ in the premise.
%   \item Apply similar logic for cases like the above where extra $\emp$s appear as conjuncts around the main conclusion formula.
%   \end{enumerate}
% \item If this was not an easy case, try harder.
%   \begin{enumerate}
%   \item Introduce names for all existentially-quantified variables in the premise.
%   \item Introduce names for the values hidden inside of all computationally-irrelevant variables in scope.
%   \item Try applying \cd{unfolder}.  If it succeeds, repeat from the introduction of names.
%   \item Check if the conclusion matches the precondition of the rule for memory reads, with a unification variable $U$ appearing for the frame assertion.  If so, pick out from the conclusion the pointer $p$ that is being read.  Search syntactically in the premise for a points-to fact $\pts{p}{v}$.  ``Cross out'' that fact, and set $U$ equal to a function that abstracts over all occurrences of $v$ in what remains of the premise.  The function must additionally re-quantify any variables coming from existential quantifiers, and it must explicitly unpack any computationally-irrelevant variables that appear.
%   \item If the last rule did not match, check if the conclusion matches the precondition of the rule for memory writes for some pointer $p$, with an extra unification variable $U$ added as a frame assertion.  Like in the last case, pick out a premise assertion $\pts{p}{v}$ and remove it.  Set $U$ equal to everything that remains of the premise, modulo the same re-quantification as in the last case.
%   \item If the previous two rules did not match, cancel out equal subformulas on both sides of the implication.  If only a unification variable $U$ remains in the conclusion, set it equal to the remaining premise, after re-quantification.
%   \end{enumerate}
% \item Loop through attempting the following premise-oriented simplifications, until no more apply.
%   \begin{enumerate}
%   \item Apply standard Coq automation tactics, covering propositional structure and simplification of pattern-matching and calls to recursive functions.
%   \item Apply \cd{solver}.
%   \item Remove an occurrence of $\emp$ from the premise.
%   \item Remove a pure fact $[p]$ from the premise, adding it as a hypothesis to the normal proof context.
%   \end{enumerate}
% \item Loop through attempting the following conclusion-oriented simplifications, until no more apply.  Try the first two steps of the last loop after each simplification.
%   \begin{enumerate}
%   \item Replace an existentially-quantified formula with its body, where the bound variable is replaced by a fresh unification variable.
%   \item Cancel out a pair of occurrences of the same subformula on both sides of the implication.  The unification that occurs in this step can induce settings for unification variables introduced by the previous rule.
%   \item Remove an occurrence of $\emp$ from the conclusion.
%   \item Find an occurrence of a pure fact $[p]$ in the conclusion, prove it with \cd{solver}, and remove it from the conclusion.
%   \end{enumerate}
% \item If the sides of the implication match, finish the proof by reflexivity.
% \end{enumerate}

\subsection{Standard Coq Automation}

When \cd{sep} has run out of rules to apply, the remaining subgoal is subjected to standard Coq automation.  Propositional structure and calls to recursive functions are simplified where possible.  \cd{sep} ends by running a loop over those simplifications and the simplifications performed by the user's \cd{solver} tactic, until no further progress can be made.  Finally, \cd{sep} discharges all goals of the form \cd{P ==> P}, by reflexivity.

Every step of the overall process is implemented in Ltac, so that only a bug in Coq would allow \cd{sep} to declare an untrue goal as true, no matter which customization the programmer provides.  By construction, every step builds an explicit proof term, which we can be validated afterward with an independent checker that is relatively simple, compared to the operation of all of the decision procedures that may have contributed to the proof.


%% \section{Comparison to Previous Ynot (AVI)}

%%  The original formulation of Ynot lead to large, unwieldy proofs.
%%  Reasoning about separation logic connectives was done at the heap
%%  level; a larger fraction of all of our proof scripts simply massaged
%%  subeaps into appropriate forms.  We had some tactics that implemented
%%  basic operations, such as the split, join, and flattening of
%%  sub-heaps, but the proof writer was forced to stitch them together to
%%  force the heaps into the correct form.

%%  The focus of our re-implementation of the Ynot system has been proof
%%  automation.  Throught the use of the sep tactic, we have virtually
%%  eliminated the need to reason about heaps; reasoning is done at the
%%  level of seperation logic.  Much of this reasoning has in turn been
%%  automated by a set of tactics such as \texttt{sep auto}. These
%%  tactics reduce the need to reason about the connectives of separation
%%  logic and their properties, allowing the proof writer to focus on the
%%  domain specific parts of the proof.  For proofs that only have simple
%%  domain specific parts, \texttt{sep auto} is often able to prove them
%%  on its own.

%%  The new model uses ``ghost variables'' to simplify specifying
%%  properties. It does so in a way that guarantees that they are
%%  computationally irrelevant, and so can always be eliminated when a
%%  program is extracted.  This in turn allows the new model to use unary
%%  post-conditions, which greatly simplifies specifications and enables
%%  the greater automation.  The old model used binary post-conditions to
%%  obviate the need for ghost variables.  Binary post-conditions where
%%  generally harder to reason about, since the connection between pre-
%%  and post-conditions always needed to be re-established.  In the new
%%  style, the connection is built in to the specification.

%%  In the new Ynot, proofs are an order of magnitude shorter then they
%%  where in the old Ynot. (provide numbers here)

\section{Evaluation}

We have used our environment to implement and verify a several data
structures, including the Stack and Queue examples that appeared in
Section \ref{tutorial}.  We also follow the prior Ynot system in
implementing a generic signature of imperative finite maps.  We built
three very different implementations of that signature: a trivial
implementation based on pointers to heap-allocated functional
association lists, an implementation based on binary search trees, and
an implementation based on hash tables.  Any of the implementations
can be used interchangeably via ML-style functors, and their shared
signature is phrased in terms of dependently-typed maps, where the
type of data associated with a key may be calculated from an arbitrary
Coq function over that key.

We also verified one more exotic data structure: binomial trees, which
are tree structures with a non-trivial rule for determining how many
pointers are stored at each node.  This data structure is often
applied in implementing priority queues.  Our implementation is
interesting in its use of a dependently-typed recursive function to
characterize functional models of such trees.

Finally, we chose representative examples from two competing data
structure verification systems, Smallfoot~\cite{smallfoot} and
Jahob~\cite{jahob}, and reimplemented those examples in our new Ynot.
For Smallfoot, we worked with one of their example inputs that gives
10 functions over imperative linked list segments, with enough
specifications to enable memory safety verification.  Our alternative
version goes further and verifies functional correctness.  We have a
more direct comparison with the Jahob example of an association list
class, where we again reimplement all of their methods in the new Ynot
style.

Our experience developing Ynot has resulted in hard-won practical
lessons about proof engineering and Coq pragmatics.  Our Coq tactics
and development methodology are directly based on this experience.
For instance, the first parameter to {\tt sep}, which performs
definition unfolding, was specifically added to abstract a common,
natural design pattern.

Our automation requires the programmer to prove relatively mechanical
lemmas about definition unfolding.  Instead, a Ynot programmer may use
explicit annotations, in the form of framing conditions and C-style
``asserts.''  Even in our examples, we have found that annotations
tend to hinder reusability.  However, it is generally possible to
trade off effort between annotation-writing and automation
construction.  It is sometimes convenient to separate the programming
and automation aspects of development, sketching code with assertions
that will be removed later as the programmer discovers which
automation would be effective.

Figure \ref{loc} presents code size statistics for our case studies.
For comparison to the original, manual-proof-oriented Ynot system, we
provide code size statistics for two of our examples as they were
implemented in that system.

Canonical Ynot modules in the new system have a particular structure
which is reflected in the column headings: specifications, heap
representation, proofs, tactics, and program code.  Ynot modules begin
with two sections that we ignore in our statistics: first, an ADT
module type is defined, and then a pure functional model, including
proofs and tactics, is defined or imported.  We ignore the module type
definitions because they would be double counted in the specifications
column, which includes the pre and post conditions of every function
defined in the module.  We do not include such functional code in the
statistics, because it is generally unsurprising that particular
theorems about pure functions are provable easily in modern proof
assistants; the overhead for these theorems will tend to remain
constant, no matter which approach one adopts to imperativity.  The
core of a Ynot module, then, consists of heap representation code
(e.g. \cd{rep}), and proofs (e.g. \cd{push\_listRep}) and tactics
(e.g. {\cd{simp\_prem}) dealining with these representations; these
three columns, plus the annotations column, are totaled in the
overhead column.  The annotations column counts the number of lines of
programmer specified annotations (e.g. \cd{<@>}). Program code
(e.g. \cd{getElements}) is code that is preserved by extraction.  Coq
type-checking and proving times (in minutes and seconds) were measured
on a 2.8 GHz Pentium D with 1 GB of memory.  So far, we have not
optimized our tactics for running time; they are executed by direct
interpretation of source code in a dynamically-typed language, so
there is much opportunity for improvement there.  Some of our examples
also contains formalizations of similar data structures that could
profitably share representation and tactic code, though we have not
taken advantage of that opportunity yet.

\begin{figure*}
  \begin{center}
    \begin{tabular}{r | r | r | r | r | r | r | r | r}
                                             & Program & Specs & Rep & Proofs & Tactics &  Annotations    & Total Overhead & Time (m:s) \\ \hline
Stack                                        &         &       &     &        &         &                 &             & 0:12 \\
Queue                                        &         &       &     &        &         &                 &             & 1:36 \\
Ref to Functional Finite Map                 &         &       &     &        &         &                 &             & 0:05 \\
Hash Table                                   &         &       &     &        &         &                 &             & 0:45 \\
Hash Table (old)                             &         &       &     &        &         &                 &             &      \\
BST Finite Map                               &         &       &     &        &         &                 &             & 1:35 \\
BST Finite Map  (old)                        &         &       &     &        &         &                 &             & 1:35 \\
Binomial Tree                                &         &       &     &        &         &                 &             & 2:33 \\
Association List                             &         &       &     &        &         &                 &             & 3:10 \\
Linked List Segments                         &         &       &     &        &         &                 &             &      \\
    \end{tabular}
  \end{center}
\footnotesize
\normalsize
  \caption{\label{loc}Breakdown of numbers of lines of different kinds of code in the case studies}
\end{figure*}

Our examples indicate that Ynot compiles quickly enough to be useful in practice, and that a variety of tradeoffs between columns are possible.  Development is done interactively with Coq, so programmers are typically only exposed to shorter, single-method verification times.  From our data, it seems plausible that, for a variety of implementations, we can maintain a small constant factor of verification overhead beyond conventional program size.  To better explain how our kinds of overhead compare to those found in related systems, we turn to our two comparison studies.

\subsection{Jahob}

The Jahob system allows the specification and verification 
of recursive, linked data structures in a fragment of Java.  Ynot and Jahob 
both relate the abstract state of a data structure to its concrete heap representation
using specification variables and pre-post conditions; both systems use
programmer supplied annotations to guide the verification process.  The systems 
differ in their heap models and how verification conditions are discharged.

Jahob specifications are written in Java comments using classical higher order
logic and set theory extended with useful operators over sets and heaps (e.g., 
transitive-closure, is-tree).  Instead of using a fixed a set of operators and
the low-level language of set theory, Ynot's separation logic layer lets 
programmers build their own abstractions using the full expressiveness of Coq.  
 
Jahob programs are verified by first being translated into an extended guarded command language.
Then, Jahob generates verification conditions 
 that are discharged using a combination of external theorem provers: if one prover does not succeed 
on an obligation, another is tried.  This allows Jahob to run multiple
provers in parallel.  It also means that practically speaking, verification 
and development are not programmer-interactive.

Because provers are often specialized to particular
classes of formulas, formula approximation techiques must be used to integrate them with
a system using higher order logic. Jahob supports a number of first order and SMT provers,
Isabelle and Coq, and also the MONA prover for the monadic fragment of 2nd order logic.  
In contrast, Ynot's automated reasoning is implemented completely in Coq.  
The ``toolchain overhead'' of using external provers is high: the 
entire Ynot codebase is smaller than just the Jahob-MONA interface.

We have compared the Jahob association list example with an equivalent
in Ynot.  The Ynot program is X larger, but it not only contains
functionality that is implemented by the Jahob backend, it also
contains reasoning code that can presumably be shared with similar
datastructures.  Modulo these differences, the Jahob and Ynot examples
are, perhaps unsurprisingly, quite similar.  For instance, they both
require large specifications for loop invariants -- in the remove
method, for instance, Jahob uses X lines of such specs and Ynot uses Y
lines.  Our Ynot implementation uses explicit framing conditions in
places where Jahob does not, but we speculate that we can probably
remove these annotations with some additional custom automation.

In practice, specifications have a different feel in Ynot than in Jahob.  One of
the hard lessons we have learned is that for our purposes, in Coq, 
it is more effective to reason ``by computation'' about inductive objects like lists
than to reason ``by logic'' about extensional objects like sets.  For this reason,
our equivalent Ynot implementation
uses lists with a uniqueness invariant to provide the semantic guarantees
of the Jahob interface.

\subsection{Smallfoot}

Our second comparison is against the Smallfoot~\cite{smallfoot}
system, which focuses on completely automatic reasoning and
concurrency.  We have not yet tackled concurrency in our system, so
our comparison focuses on automatic reasoning about sequential
programs.  We compare two aspects of the systems: the size of
annotations and proof obligations and the expressivity of the
logics. We use linked list segments as a running example.

The Smallfoot system defines a segment of a linked list as the minimal
predicate satisfying the following:
$$
ls(E,F) \Leftrightarrow (E = F \wedge \mathtt{emp}) \vee (E \neq F \wedge \exists y. \; E.tl \mapsto y \; * \; ls(y,F))
$$
This is almost the same definition as we gave earlier in the Queue example, with different notational conventions.  The notable difference is that we add the pointer inequality, which is necessary to guarantee lack of cycles.

To compare annotatation burden, we consider an {\tt append} function
on lists as a representative example. In Smallfoot, the function is
built using a {\tt while} loop and requires 14 lines and three
annotations: the pre- and post-conditions and the loop invariant. To
maintain a close symmetry between the definitions, we break the {\it
append} function into two pieces, one for the loop and the other for
the main body. Our two Ynot functions for these pieces are,
respectively, 4 and 5 lines long and require 2 lines of annotations
for the fixpoint definition, corresponding to the Smallfoot loop
invariant. As with all previous examples, the tactics and lemmas used
for this function apply to all functions dealing with linked lists, in
the same way that the linked list representation predicate is
axiomatized and reused in Smallfoot.

While the Smallfoot system is faster and requires less verbose
annotations, Ynot has two primary advantages over it. The first is
that, as previously mentioned, Ynot supports functional correctness
verification, while Smallfoot guarantees only heap-safety. Consider
the type of the list {\it cons} function.  Smallfoot can only
guarantee that the returned list is valid, not that it really has a
new element pushed on front.  {\it cons}, {\it removeFirst}, and other
list operations share a single return value specification, despite the
fact that they are meant to do different things.

The second advantage in Ynot is the ability to express properties in
higher-order logic. Smallfoot's ease of use comes at the price of
restricting programs to a small set of data structures (including
linked lists, doubly linked lists, trees, and xor-lists) for which the
definitions and corresponding inductive properties can be defined
using predicate logic. The theory built for each data structure in
Smallfoot plays the same roll as the unfolding lemmas that any user of
Ynot may prove for new data structures.  Figure \ref{loc} includes the
size of the lemmas and proofs of that kind that we wrote for this
example and the others.

\section{Related Work}

The ESC/Java~\cite{esc-java} and Spec\#~\cite{spec-sharp} systems tackle some related problems within the classical verification framework.  These systems have strictly less support for modeling data structures than Jahob has, so that it is impractical use them to perform full verifications of many data structures.

A number of systems have been proposed recently to support dependently-typed programming in a setting oriented more towards traditional software development than Coq is.  ATS~\cite{ats} includes novel means for dealing with imperative state, but it includes no proof automation beyond decision procedures for simple base theories like linear arithetic, making it much harder to write verified data structure implementations than in our system.  Concoqtion~\cite{concoqtion} allows the use of Coq for reasoning about segments of general OCaml programs.  While those programs may use imperativity, the Coq reasoning is restricted to pure ``index terms.''  Sage~\cite{sage} supports ``hybrid type-checking,'' where typing invariants may be specified with boolean-valued program functions and checked at runtime.  This approach generally does not enable full static correctness verification.

There is closely related work in the field of shape analysis.  The TVLA system~\cite{tvla} models heap shapes with a first-order logic with a built-in transitive closure operation.  With the right choices of predicates that may appear in inferred specifications, TVLA is able to verify automatically many programs that involve both heap shape reasoning and reasoning in particular decidable theories like arithmetic.

The Xisa system~\cite{xisa} uses an approach similar to ours, as Xisa is based on user specification of inductive characterizations of shape invariants.  Xisa builds this inductive definition mechanism into its framework, while we inherit a more general mechanism from Coq.  Xisa is based on hardcoded algorithms for analyzing inductive definitions and determining when and how they should be unfolded.  Such heuristics lack theoretical guarantees about how broadly they apply.  In the design of our system, we recognize this barrier and allow users not just to define new inductive predicates, but also to extend a generic solver with arbitrary rules for simplifying uses of the predicates.

In comparing the new Ynot environment to the above systems and all others that we are aware of, there are a number of common advantages.  No other system supports both highly-automated proofs based on separation logic (when they work) and highly human-guided proofs (when they are needed), let alone combinations of the two.  None of the systems with significant automation support the combination of imperative and higher-order features, like we handle in the example of our higher-order memoizer and iterators.  We also find no automated systems that deal with dependent types in programs.  The first of these advantages seems critical in the verification of imperative programs that would be difficult to prove correct even if refactored to be purely functional.  For instance, it seems plausible that our environment could be used eventually to build a verified compiler that uses imperative data structures for efficient dataflow analysis, unification in type inference, and so on.  None of the purely-automated tools that we have surveyed could be applied to that purpose without drastic redesign.  We are not aware of any previous toolkit for manual proof about imperative programs in proof assistants that would make the task manageable; the manual reasoning about state would overwhelm ``the interesting parts'' of compiler verification.


\section{Conclusions \& Future Work}

Coq has not proved to be the ideal setting for reasoning with specification variables.  Our encoding of computationally-irrelevant variables is in terms of the \cd{inhabited} type from Coq's standard library, and we need to assume an axiom which states that the constructor for building irrelevant values is injective.  This is not a fundamental limitation; we hope for a future version of Coq or a related system to include general support for computationally-irrelevant values.

Concurrency is another big area for future work.  Systems like Smallfoot~\cite{smallfoot} do automated separation-logic reasoning about memory safety of concurrent programs.  We would like to extend that work to full correctness verification, by designing a monadic version of concurrent separation logic that fits well within Coq.

The full potential of the Ynot approach also depends on explicit handling of other computational effects, like exceptions and input-output.  We speculate that it would be relatively straightforward to add these effects to our implementation, with most of the hard work of building automation machinery transferring directly.

As with any project in automated theorem proving, there is always room for improvements to automation and inference.  Several tools have been successful at inferring separation logic invariants using abstract interpretation.  We have held off on such approaches so far, since it is easiest to implement a system like ours by phrasing inference as a theorem-proving problem.  A future version of Ynot could benefit greatly in usability by incorporating abstract interpretation somehow.

Nonetheless, our current system already fills a crucial niche in the space of verification tools.  We have presented the first tool that performs well empirically in allowing mixes of manual and highly-automated reasoning about heap-allocated data structures, as well as the first tool to provide aggressive automation in proofs of higher-order, imperative programs.  We hope that this will form a significant step towards full functional verification of imperative programs with deep correctness theorems.

%\bibliographystyle{plainnat}
\bibliographystyle{plain}

\bibliography{bib}

\end{document}

% LocalWords:  Coq Hoare Ynot checkable dereference ESC Concoqtion logics Coq's
