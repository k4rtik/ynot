\documentclass[preprint,nocopyrightspace]{sigplanconf}
%\usepackage[square, comma, sort&compress]{natbib}
\usepackage{proof}

\usepackage{amsmath}
\usepackage{color}

\newcommand{\todo}[1]{\textcolor{red}{#1}}

\newcommand{\coq}[1]{\mathsf{#1}}
\newcommand{\return}[1]{\coq{return}(#1)}
\newcommand{\bind}{\leftarrow}
\newcommand{\emp}{\mathbf{emp}}
\newcommand{\sep}{\ast}
\newcommand{\pts}[2]{#1 \mapsto #2}
\newcommand{\new}[1]{\coq{new}(#1)}
\newcommand{\free}[1]{\coq{free}(#1)}
\newcommand{\rd}[1]{!#1}
\newcommand{\wri}[2]{#1 := #2}
\newcommand{\himp}{\Rightarrow}

\begin{document}

%\conferenceinfo{PLDI ’09}{ todo }

%\copyrightyear{2005}

%\copyrightdata{1-59593-056-6/05/0006}

\preprintfooter{DRAFT}

%\titlebanner{DRAFT}

\title{Effective Interactive Proofs for Imperative Programs}


\authorinfo{Double blind}
{ }

\maketitle

\begin{abstract}
  We present a redesign and reimplementation of the Ynot system for programming and verification of imperative programs in the Coq proof assistant.  We begin the ``practice'' part of the ``theory''-``practice'' pipeline, showing how the theory of Ynot can be used to craft a practical development environment, where it is possible to do full functional verification of imperative data structures with reasonable programmer effort.  For most implementations, the verification burden is reduced by at least an order of magnitude compared to the old Ynot system, replacing manual proof with automation.  The core of the automation is a simplification procedure for implications in higher-order separation logic, with hooks that allow programmers to add domain-specific simplification rules.
  
  Compared to competing approaches to data structure verification, our system includes much less code that must be trusted; namely, about a hundred lines of Coq code defining a program logic.  All of our theorems and decision procedures have or build machine-checkable correctness proofs from first principles, removing opportunities for tool bugs to create faulty verifications.  We argue for the effectiveness of our infrastructure by verifying a number of data structures and comparing to similar efforts within other projects.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\terms
%term1, term2

%\keywords
%keyword1, keyword2

\section{Introduction (GREG)}
\begin{itemize}
\item Want to verify imperative programs
\item Higher-order functions
\item modularity
\item {\it mom and apple pie}
\end{itemize}

  Ynot is the right design from a modularity, re-use perspective.  Much of the success hinges on the use of a higher-order dependently-typed language (Coq) and the ability to smoothly integrate modeling (inductive definitions), domain-specific abstractions (e.g., STsep) and uniform abstraction (Pi). If Ynot is to succeed, need drastic improvements in automation.


\section{Overview (GREG)}
\begin{itemize}
\item Features of Ynot
\item problem with the proofs
\end{itemize}


\section{The Ynot Programming Environment}

The axiomatic base of Ynot is a fairly standard Hoare logic.  The main difference of our logic from usual presentations is that it is designed to integrate well with Coq's functional programming language, so that we formalize a language of expressions instead of commands.  A program derivation is of the form $\{P\} \; e \: \{Q\}$, where $P$ is a precondition predicate over heaps, and $Q$ is a postcondition predicate over initial heaps, functional values of $e$, and final heaps.  For instance, we can derive
$$\{\lambda \_. \; \top\} \; \return{1} \; \{\lambda h, v, h'. \; h' = h \land v = 1\}$$
and
$$\begin{array}{c}
  \{\lambda h. \coq{sel}(h, p_1) = p_2\} \\
  x \bind \; \rd{p_1}; \wri{x}{1} \\
  \{\lambda h, \_, h'. \; \coq{sel}(h, p_1) = p_2 \land h' = \coq{upd}(h, p_2, 1)\}
\end{array}$$

In actuality, Ynot has no separate concepts of programs and derivations.  Rather, the two are combined into one dependent type family, whose indices give the specification of a program.  For instance, the type of the first example program above would be:
\begin{verbatim}
ST (fun _ => True) (fun h v h' => h' = h /\ v = 1)
\end{verbatim}

Heaps are represented as functions from pointers to dynamically-typed packages, which are easy to implement in Coq with an inductive type definition.  The pointer read rule enforces that the heap value being read has the type that the code expects.  The original Ynot paper~\cite{ynot:icfp} contains further details of the base program logic.


\section{A Derived Separation Logic}

\begin{figure*}
  $$\infer{\{\emp\} \; \return{v} \; \{\lambda v'. \; [v = v']\}}{}
  \quad \infer{\{P_1\} \; x \bind e_1; e_2 \; \{Q_2\}}{
    \{P_1\} \; e_1 \; \{Q_1\}
    & (\forall x, \{P_2(x)\} \; e_2 \; \{Q_2\})
    & (\forall x, Q_1(x) \himp P_2(x))
  }$$

  $$\infer{\{\emp\} \; \new{v} \; \{\lambda p. \; \pts{p}{v}\}}{}
  \quad \infer{\{\exists v, \pts{p}{v}\} \; \free{p} \; \{\lambda \_. \; \emp\}}{}$$

  $$\infer{\{\exists v, \pts{p}{v} \sep P(v)\} \; \rd{p} \; \{\lambda v. \; \pts{p}{v} \sep P(v)\}}{}
  \quad \infer{\{\exists v, \pts{p}{v}\} \; \wri{p}{v'} \; \{\lambda \_. \; \pts{p}{v'}\}}{}$$

  $$\infer{\{P\} \; e \; \{Q\}}{
    P \himp P'
    & \{P'\} \; e \; \{Q'\}
    & Q' \himp Q
  }
  \quad \infer{\{P \sep R\} \; e \; \{Q \sep R\}}{
    \{P\} \; e \; \{Q\}
  }$$

  \caption{\label{STsep}The derived separation logic}
\end{figure*}

Direct reasoning about heaps leads to very cumbersome proof obligations, with many sub-proofs that pairs of pointers are not equal.  Separation logic~\cite{separation} is the standard tool for reducing this complexity.  The old Ynot system built a separation logic on top of the axiomatic foundation, and we do the same here.  As before, we introduce no new syntactic class of separation logic formulas.  Instead, we define functions that operate on arbitrary predicates over heaps, with the intention that we will only apply them on separation-style formulas.  Nonetheless, it can be helpful to think of our ``assertion language'' as defined by:
$$\begin{array}{rcl}
  p &::=& [P] \mid \emp \mid \pts{x}{y} \mid p \sep p \mid \exists x, p
\end{array}$$

For any pure Coq proposition $P$, $[P]$ is the heap predicate that asserts that $P$ is true and the heap is empty.  $\emp$ asserts that the heap is empty, and $\pts{x}{y}$ asserts that the heap contains only a mapping from $x$ to $y$.  $p_1 \sep p_2$ asserts that the heap can be broken into two heaps $h_1$ and $h_2$ with disjoint domains, such that $h_1$ satisfies $p_1$ and $h_2$ satisfies $p_2$.  Finally, we add existential quantification.

It is worth pointing out that we simplify the assertion language substantially by taking advantage of Coq's base language.  We do not need to include program variables in the logic, because any Coq variable may be included anywhere in a Coq development, including within one of our assertions.  The same argument allows us to avoid explicit mention of specification variables, sometimes also called ``ghost state'' variables.

We can write much more expressive formulas than in most systems based on separation logic.  Not only can any pure proposition be injected with $[\cdot]$, but we can also use arbitrary Coq computation to build impure assertions.  For instance, we can include calls to custom recursive functions that return assertions.  We need no special support in the assertion language to accommodate this, and Coq's theorem-proving support for reasoning about pattern-matching recursive functions can be brought to bear without modification.

This automatic importation of Coq features has some surprising and pleasant consequences.  We literally need no more of the standard separation logic connectives to verify a wide variety of data structures.  For instance, separation logic proofs often use several kinds of disjunction.  In our setting, we reduce this disjunction to pattern matching over datatypes.  We take advantage of Coq's computational rules for simplifying pattern matches, which need not mention heaps, making them easier to think about and promoting easier proof automation.

We do need a notion of implication, though we do not need to include it in assertions.  There is no need to introduce the ``magic wand'' of separation logic.  Rather, we just define an almost trivial operator $\himp$, which appears only at the top level of individual proof obligations.  The meaning of $p \himp q$ is that any heap satisfying $p$ also satisfies $q$.  The relevant aspects of the usual ``magic wand'' connective are built into the definition of what a valid program derivation is in the separation logic.

\medskip

What we have described so far is the same as in the original Ynot work, with the exception that that work used some additional separation connectives that we no longer need.  The big departure of our new system is that we define a more standard separation logic.  The old Ynot's separation logic included ``binary postcondtions'' that may refer to both the initial and final heaps.  This is in stark contrast to traditional separation logics, where all assertions are simple separation formulas, and all verification proof obligations are simple implications between such assertions.  The utility of that formalism has been born out in the wealth of tools that have used separation logic for automated verification.  In contrast, verifications with the old Ynot tended to involve at least tens of lines of manual proof per line of program code.

Why did the original Ynot use this nonstandard program logic?  The answer has to do with the need for an effective analogue of specification variables.  In traditional separation logic, specification variables are commonly used to ensure that parts of state are preserved by commands, when the same specification variable appears in both the precondition and postcondition of a derivation.  In contrast, the old Ynot used binary postconditions for the same purpose, asserting equations between parts of the pre and post heaps.

The final output of a Ynot program is an executable piece of OCaml or Haskell code, produced with Coq's program extraction facility.  We can use standard Coq variables as specification variables, but, by default, they will remain at runtime, reducing the efficiency of programs.  One of the main innovations of the new work we present involves a new way of encoding specification variables in Coq, such that they will be erased by the standard program extraction facility.  This makes it possible to use a more standard separation logic, with the corresponding decrease in the difficulty of automating proofs.  We will present this new technique by example in later subsections.




\section{Comparison to Previous Ynot (AVI)}

 The original formulation of Ynot lead to large, unwieldy proofs.
 Reasoning about separation logic connectives was done at the heap
 level; a larger fraction of all of our proof scripts simply massaged
 subeaps into appropriate forms.  We had some tactics that implemented
 basic operations, such as the split, join, and flattening of
 sub-heaps, but the proof writer was forced to stitch them together to
 force the heaps into the correct form.

 The focus of our re-implementation of the Ynot system has been proof
 automation.  Throught the use of the sep tactic, we have virtually
 eliminated the need to reason about heaps; reasoning is done at the
 level of seperation logic.  Much of this reasoning has in turn been
 automated by a set of tactics such as \texttt{sep auto}. These
 tactics reduce the need to reason about the connectives of separation
 logic and their properties, allowing the proof writer to focus on the
 domain specific parts of the proof.  For proofs that only have simple
 domain specific parts, \texttt{sep auto} is often able to prove them
 on its own.

 The new model uses ``ghost variables'' to simplify specifying
 properties. It does so in a way that guarantees that they are
 computationally irrelevant, and so can always be eliminated when a
 program is extracted.  This in turn allows the new model to use unary
 post-conditions, which greatly simplifies specifications and enables
 the greater automation.  The old model used binary post-conditions to
 obviate the need for ghost variables.  Binary post-conditions where
 generally harder to reason about, since the connection between pre-
 and post-conditions always needed to be re-established.  In the new
 style, the connection is built in to the specification.

 In the new Ynot, proofs are an order of magnitude shorter then they
 where in the old Ynot. (provide numbers here)
\section{Evaluation}
\begin{figure*}
\begin{center}
\begin{tabular}{r | r | r | r | r | r | r | r}
 & lines of code & total  non-code & annotations & lemmas & proofs & aux. def & tactics\\\hline
{\tt LL-ref} & AVI\\
{\tt LL-Jahob} & RYAN\\
{\tt LL-Smallfoot} & GREGORY\\
{\tt Hashtable} & AVI\\
{\tt Binary Heap} & RYAN\\
{\tt BST} & ??\\
{\tt Stack} & ADAM\\
{\tt Queue} & ADAM\\
\end{tabular}
\end{center}
\end{figure*}

\section{Related Work}
\subsection{Jahob (RYAN)}

The Jahob system~\cite{jahob} allows the specification and verification 
of recursive, linked data structures in a fragment of Java.  Like ynot, 
verified programs are correct up to termination.  

Jahob specifications are written in classical higher order logic.  
The abstract state of an object is given by s a collection of 
programmer defined ``specvars'' which do not exist during program execution.
Specvars are defined by Jahob formulae, which resemble formulae of set 
theory in Isabelle/HOL.  Jahob formulae are simply typed with ground types
bool, obj, and int, type constructors $\Rightarrow$ for total functions, * for tuples
and set for sets.  The logic contains polymorphic equality, standard
logical connectives $\wedge, \vee, \neg, \to, \forall, \exists$ and
$\lambda$ binders, set comprehension, operations on sets ($\cup, \in$) 
and transitive closure, finite set cardinality, and a tree function for indicating
that a structure is a tree.  Here is the interface for a Jahob association list: 

\begin{verbatim}
class AssocList {
//: public specvar content :: "(obj * obj) set
public Object put(Object k0, Obkect v0)
/*: requires "k0 <> null /\ v0 <> null"
    modifies content
    ensures  
    "content = old content 
               - {(k0, result)} U {(k0, v0)} 
     /\ (result =  null -> 
          ~ exists v, 
             (k0, v) in old content) /\
     /\ (result <> null -> 
           (k0, result) in old content) */
{...}
public Object get(Object k0)
/*: requires "k0 <> null"
    ensures  
    "result =  null -> 
      ~ exists v, (k0, v) in content 
      /\ result <> null -> 
           (k0, result) in content" */
}
\end{verbatim}

Like ynot, Jahob relates the abstract state of a data structure to its concrete heap representation.  
One way to implement the association list with a singly linked list is to represent links between 
nodes using an edge relation (vardef is a shorthand definition):

\begin{verbatim}
public /*: claimedBy AssocList */ 
class Node {
    public Object key; public Object value; 
    public Node next;
    //: public ghost specvar cnt 
         :: "(obj * obj) set" = "{}"
}
private static specvar edge 
  :: ``obj => obj => bool'';

vardefs ``edge == (fun x y => 
  (x in Node /\ y = x..next) \/
  (x in AssocList /\ y = x..first))'';
invariant InjInv: forall x1 x2 y,
   y <> null /\ edge x1 y 
   /\ edge x2 y -> x1 = x2'';
\end{verbatim} 

The (recursive) representation invariant defines the members of the abstract set 
representing the linked list as the union of the element in the head node 
and the members of the subsequent nodes.  It also ensures that keys only occur once in the list.

\begin{verbatim}
private Node first;

vardefs "content == first..cnt";

invariant CntDef:
    ``forall x, x in Node /\ x in alloc 
        /\ x <> null ->
           x..cnt = {(x..key, x..value)} 
              U x..next..cnt /\
           (forall v, (x..key, v) 
              notin x..next..cnd)'';
invariant CntNull:
    ``forall x, x in Node /\ x in Alloc 
        /\ x = null -> x..cnt = {}'';
\end{verbatim}

Ynot's approach to specification and abstract state is similar in spirit to Jahob's but 
feels more computational in practice.
Because ynot is embedded in Coq, specifications and programs share a common language.
This makes it possible to define specifications using programs with reduction behavior
rather than with formulae of set theory; reasoning about such computational entities
is at the core of ynot's proof automation.  In addition, ynot uses separation connectives
to describe the heap.  For instance, the representation invariant rep for the ynot
singly linked list implementation is defined as a Coq fixpoint of essentially a separation logic formula:

\begin{verbatim}
Variables K V: Set.
Variable eqK : forall (k1 k2: K), {k1=k2} + {k1<>k2}.

Record Node : Set := node {
  key: K; value: V; next: option ptr
}.

Definition AssocList : Set := ptr.

Fixpoint rep' (m : list (prod K V)) 
(p : option ptr) {struct m} : hprop :=
  match p with
    | None => [m = nil]
    | Some hd => 
      match m with
        | (k,v) :: b => Exists nxt :@ option ptr, 
             hd --> node k v nxt * rep' b nxt
        | nil => [False]
      end
  end.

Definition rep (m: list (prod K V)) 
 (ll: AssocList) : hprop :=
   Exists n :@ option ptr, 
    ll --> n * rep' m n.
\end{verbatim}

The computational behavior of specification means, for instance, that the type of new
\begin{verbatim}
Definition new : STsep __ (rep nil).
\end{verbatim}

is definitionally equal to 
\begin{verbatim}
  STsep __ (fun p => match p with 
                       | None => [nil = nil] 
                       | Some hd => [False]
                     end)
\end{verbatim}

and so it is easy to automate (using a tactic called t) the reasoning that proves
the new association list allocator is correct:
\begin{verbatim}
Definition new : STsep __ (rep nil).
  refine {{ New None }}; t. Qed.
\end{verbatim}

The specifications for put and lookup exploit other pure functional programs:

\begin{verbatim}
Fixpoint lookup (k: K) 
  (l: list (prod K V)) : option V :=
 match l with
  | nil => None
  | (k', v)::b => if eqK k k' 
                  then Some v 
                  else lookup k b
 end.

todo - change put 
Definition put (ll : AssocList) 
 (m : [list (prod K V)]) (k : K) (v : V) :
  STsep (m ~~ rep m ll)
        (fun _:unit => m ~~ rep ((k,v)::m) ll ).

Definition get'' (k: K) (hd: option ptr) 
 (m: [list (prod K V)]) : 
  STsep (m ~~ rep' m hd)
        (fun res:option V => m ~~ 
           [res = lookup k m] * rep' m hd).
\end{verbatim} 

The actual implementations of get in ynot and jahob are essentially identical.
In both cases annotations are required to guide the proving process.  (Add numbers here).
In Jahob:

\begin{verbatim}
/*: requires ``k0 <> null''
    ensures 
      ``(result <> null -> 
          (k0, result) in content 
          /\ (result =  null -> 
                ~exists v0, 
                  (k0, v) in content)) */
{
  Node current = first;
  while //: inv ``forall v, ((k0, v) in content) 
                    = ((k0, v) in current..cnt)''
   (current != null) {
     if (current.key == k0) { 
       return current.value; 
     }
     current = current.next
  }
  return null;
}
\end{verbatim}

In ynot:

\begin{verbatim}
Definition get (k: K) 
 (hd: option ptr) (m: [list (prod K V)]):
  STsep (m ~~ rep' m hd)
        (fun res:option V => m ~~ 
           [res = lookup k m] * rep' m hd).
intro k.
refine (Fix2
    (fun hd m => m ~~ rep' m hd)
    (fun hd m o => m ~~ 
       [o = lookup k m] * rep' m hd)
    (fun self hd m =>
      IfNull hd
      Then {{ Return None }}
      Else fn <- hd !! fun fn => m ~~ 
             [head m = Some (key fn, value fn)] 
             * rep' (tail m) (next fn);
           if eqK (key fn) k
           then {{ Return (Some (value fn)) }}
           else {{ 
            self (next fn) (m ~~~ tail m) 
              <@> (m ~~ hd --> fn * 
                  [head m = Some (key fn, value fn)]) 
           }}
      )); try solve [ t | hdestruct m; t ].
Defined.

Definition get (k: K) 
 (ll: AssocList) (m: [list (prod K V)]) :
   STsep (m ~~ rep m ll)
         (fun r:option V => m ~~ 
            rep m ll * [r = lookup k m]).
intros; refine (hd <- !ll;
    Assert (ll --> hd * (m ~~ rep' m hd));;
    {{get'' k hd m <@> _}});
  t.
\end{verbatim}

Whereas ynot's automated reasoning is completely captured by Coq tactics,
Jahob programs are verified using a series of stages similar to 
a compiler pipeline.  First, Jahob programs are translated into 
an extended guarded command language.
Then, Jahob generates verification conditions which are discharged 
using a combination of theorem provers -- if one prover does not succeed 
on an obligation, another is tried.  This allows Jahob to run multiple
provers in parallel.  Because provers are often specialized to particular
classes of formulae, formula approximation techiques must be used to integrate them with
a system using higher order logic. Jahob supports a number of 1st order and SMT provers, and also the 
MONA prover for the monadic fragment of 2nd order logic and Isabelle and Coq. 

 \subsection{Smallfoot (GREGORY)}
We conclude our comparison with alternative approaches by comparing
our system to Smallfoot~\cite{small1,small2}. The focus of Smallfoot
is on compltely automatic reasoning, and it's almost complete nature
makes many correct programs go through with only the function level
annotations, a goal, but not yet a complete reality in Ynot. In
addition to the focus on automatic proofs, Smallfoot includes simple
concurrency primitives, $||$ and $\mathsf{with}\,r\,\mathsf{when}(B)
C$ with which they are able to implement a producer-consumer
model. Based on these points, we will draw a comparison in two parts:
the size of annotations and proof obligations and the expressivity of
the logic.

On the size of proof obligations, the Smallfoot system comes out
ahead, requiring no user-constructed proofs. The drawback to this is
that only inductive data of the following types are allowed: singly
linked lists, doubly linked lists, trees and xor lists. This
restriction is necessary to Smallfoot in order to make the proofs
decidable.

The annotations in the example programs are small mainly because they
use these. While the predicates are hard-coded in Smallfoot,
predicates can be coded directly by the user in Ynot. The style of
Ynot representations are to express data by mapping a ``ghost''
representation into a heap representation. This approach allows
stronger dependent types in the contracts for functions, requireing
the accompanying proof to be a proof of partial correctness rather
than simply of resource constraints. For example, consider the
function to test whether a list is empty in Smallfoot and in Ynot.
\begin{verbatim}
is_empty(r;l) [l |-> t * list(t)] {
  local p;
  p = l->p;
  if (p == NULL) { r = 1; } 
  else { r = 0; }
} [l|-> t * list(t)]
\end{verbatim}

The following function also has the same type:
\begin{verbatim}
is_empty(r;l) [l |-> t * list(t)] {
  r = 1;
} [l|-> t * list(t)]
\end{verbatim}
In Ynot, the \verb|is_empty| function can be defined as follows:
\begin{verbatim}
Definition is_empty A (ll : LinkedList)
  (m : [list A]) :
  STsep (m ~~ rep m ll) 
    (fun res:bool => m ~~ match res with
                            | nil => [res = true]
                            | _   => [res = false]
                          end).
  intros;
  refine (hd <- !ll;
          IfNull hd Then {{Return true}}
          Else {{Return false}}); sep simpl.
Qed.          
\end{verbatim}
The type guarantees that the function is implemented correctly with
respect to the underlying list.

\todo{This is analagous to the implementation of llseg presented in
  the smallfoot paper.}
For example, the following fixpoint expresses the recursive definition
of a linked list segement.  \todo{What is the best way to encode a
  linked list segment?}
\begin{verbatim}
Fixpoint llseg A (hd tl : option ptr) 
  (m : list A) {struct m} :=
  match m with 
    | nil => [hd = tl]
    | a :: b => 
      match hd with 
        | None => [False]
        | Some p => [hd <> tl] * 
            Exists nxt :@ option ptr, 
              hd --> node a nxt * llseg nxt tl b
      end
  end.
\end{verbatim}




Comparison points
\begin{itemize}
\item The use of Coq allows quantifiers in formula. (makes inference
  more difficult but allows more expressive types)
\item Ynot is capble of arbitrary inductive predicates
\item Ynot does not support concurrency which is a strength of
  smallfoot (race detection)
\item Smallfoot is very fast
\item A lot of the ``parallel examples'' don't actually use
  synchronization. These would be pretty trivial to implement in Ynot
  with a function:
\begin{verbatim}
SepParallel (r1 r2 : Type) (P_i Q_i : hprop)
  (P_e : r1 -> hprop) (Q_e : r2 -> hprop) :
     STsep (P_i) (P_e) -> STsep (Q_i) (Q_e)
  -> STsep (P_i * Q_i) (fun r:(r1 * r2) => 
               (P_e (fst r)) * (Q_e (snd r)))
\end{verbatim}
the more interesting aspect of the parallelism comes from the locking
mechanism, which could be encoded in Ynot but only after a little bit
of hacking. (smallfoot uses resource invariants to control this)
\end{itemize}

Smallfoot examples:
\begin{itemize}
\item circular list
\item doubly linked list
\item merge sort
\item ``memory manager'' (basically just a stack)
\item queue
\item tree
\item xor linked list segment
\item xdeq --- uses parallelism (reading and writing buffer)
\end{itemize}

\section{Other related work}


\section{Conclusions \& Future Work}
Irrelevance, concurrency, IO/effects

automation/annotations (loop invariant inference)

Check that bibliography is working~\cite{htt}.

%\bibliographystyle{plainnat}
\bibliographystyle{plain}

\bibliography{bib}

\end{document}
